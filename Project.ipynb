{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Chatbots for Technical Documentation\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Load and split the document](#load-and-split-the-document)\n",
    "- [Generate and store the embeddings](#generate-and-store-the-embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This project involves implementing a retrieval augmented generation (RAG) with `LangChain` to create a chatbot for\n",
    "answering questions about technical documentation. The document chosen for this assignment was the following: The European Union Medical Device Regulation - Regulation (EU) 2017/745 (EU MDR). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Install the packages and dependencies to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install -qU langchain langchain-community langchain-chroma langchain-text-splitters unstructured sentence_transformers langchain-huggingface huggingface_hub pdfplumber langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='02017R0745 — EN — 09.07.2024 — 004.001 — 1\n",
      "This text is meant purely as a documentation tool and has no legal effect. The Union's institutions do not assume any liability\n",
      "for its contents. The authentic versions of the relevant acts, including their preambles, are those published in the Official\n",
      "Journal of the European Union and available in EUR-Lex. Those official texts are directly accessible through the links\n",
      "embedded in this document\n",
      "►B REGULATION (EU) 2017/745 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\n",
      "of 5 April 2017\n",
      "on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and\n",
      "Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC\n",
      "(Text with EEA relevance)\n",
      "(OJ L 117, 5.5.2017, p. 1)\n",
      "Amended by:\n",
      "Official Journal\n",
      "No page date\n",
      "►M1 Regulation (EU) 2020/561 of the European Parliament and of the L 130 18 24.4.2020\n",
      "Council of 23 April 2020\n",
      "►M2 Commission Delegated Regulation (EU) 2023/502 of 1 December 2022 L 70 1 8.3.2023' metadata={'source': 'document.pdf', 'file_path': 'document.pdf', 'page': 0, 'total_pages': 232, 'ModDate': \"D:20240808025522+02'00'\", 'Producer': '3-Heights(TM) PDF to PDF-A Converter Shell 4.7.24.2 (http://www.pdf-tools.com)', 'Title': 'CL2017R0745EN0040010.0001.3bi_cp 1..1', 'Author': 'Publications Office', 'Subject': ' ', 'Creator': 'Arbortext Advanced Print Publisher 10.0.1465/W Unicode', 'CreationDate': \"D:20240724041003-07'00'\", 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "# Using PDF document\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "loader = PDFPlumberLoader(\"document.pdf\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "pages = loader.load_and_split(text_splitter)\n",
    "\n",
    "print(pages[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and store the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and store the embeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=pages, embedding=embeddings, persist_directory=\"db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Retrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "page 16: which have been published in the Official Journal of the European\n",
      "Union, shall be presumed to be in conformity with the requirements\n",
      "of this Regulation covered by those standards or parts thereof.\n",
      "The first subparagraph shall also apply to system or process\n",
      "requirements to be fulfilled in accordance\n",
      "page 197: used, particularly as regards sterilisation and the relevant documents;\n",
      "and\n",
      "(e) the appropriate tests and trials which are to be carried out before,\n",
      "during and after manufacture, the frequency with which they are to\n",
      "take place, and the test equipment to be used; it shall be possible to\n",
      "trace back ad\n",
      "page 168: an initial start-up phase.\n",
      "1.6. Participation in coordination activities\n",
      "1.6.1. The notified body shall participate in, or ensure that its assessment\n",
      "personnel is informed of, any relevant standardisation activities and in\n",
      "the activities of the notified body coordination group referred to in\n",
      "Article\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"Describe the use of harmonised standards\")\n",
    "\n",
    "print(len(retrieved_docs))\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(\"page \" + str(doc.metadata[\"page\"] + 1) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Athos\\anaconda3\\envs\\TAAC_proj1\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ':\\n\\nThis is an extremely important question—how can a large proportion of people who may not speak the'},\n",
       " {'generated_text': ' to encourage good use of the same product\\n\\n(2)Where there is a strong preference among experts in'},\n",
       " {'generated_text': \", including the EU's 'Duke of Limbo' regulations as a 'guarantee that the countries\"},\n",
       " {'generated_text': ', one based on international practice of the EU.\\n\\n1. A national harmonised standard: whether under'},\n",
       " {'generated_text': '. I find that most of the information I have for a particular type of document has nothing to do with the'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2', max_length=1000, pad_token_id=50256, return_full_text=False)\n",
    "\n",
    "gpt2 = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "set_seed(42)\n",
    "generator(\"Describe the use of harmonised standards\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['content', 'additional_kwargs', 'response_metadata', 'type', 'name', 'id', 'example', 'tool_calls', 'invalid_tool_calls', 'usage_metadata'])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "LLM stands for **Large Language Model**. It's a type of artificial intelligence (AI) that excels at understanding and generating human-like text. \n",
       "\n",
       "Here's a breakdown:\n",
       "\n",
       "**What it is:**\n",
       "\n",
       "* **A complex neural network:** LLMs are built using deep learning techniques, specifically neural networks with many layers.\n",
       "* **Trained on massive text datasets:** These models are trained on enormous amounts of text data, like books, articles, code, and online conversations. \n",
       "* **Capable of various language tasks:** LLMs can perform a wide range of tasks, including:\n",
       "    * **Text generation:** Writing stories, poems, articles, emails, and more.\n",
       "    * **Translation:** Converting text from one language to another.\n",
       "    * **Summarization:** Condensing large amounts of text into concise summaries.\n",
       "    * **Question answering:** Providing answers to questions based on given text.\n",
       "    * **Code generation:** Writing code in various programming languages.\n",
       "    * **Dialogue generation:** Engaging in natural-sounding conversations.\n",
       "\n",
       "**Examples:**\n",
       "\n",
       "* **GPT-3 (Generative Pre-trained Transformer 3):** Developed by OpenAI, it's known for its impressive text generation capabilities.\n",
       "* **BERT (Bidirectional Encoder Representations from Transformers):** Developed by Google, it excels at understanding the context of words in a sentence.\n",
       "* **LaMDA (Language Model for Dialogue Applications):** Developed by Google, it's designed for natural and engaging conversation.\n",
       "\n",
       "**Key features:**\n",
       "\n",
       "* **Generative:** LLMs can create new text based on the patterns they learned during training.\n",
       "* **Contextual:** They can understand the meaning of words based on their surrounding context.\n",
       "* **Versatile:** LLMs can be adapted to various language tasks.\n",
       "\n",
       "**Limitations:**\n",
       "\n",
       "* **Bias and misinformation:** LLMs can reflect the biases present in their training data, leading to potentially harmful or misleading outputs.\n",
       "* **Lack of true understanding:** Despite their fluency, LLMs don't truly understand the meaning of the text they generate.\n",
       "* **Computational demands:** Training and running LLMs require significant computational resources.\n",
       "\n",
       "**Overall, LLMs are powerful tools with the potential to revolutionize how we interact with language. However, it's important to be aware of their limitations and use them responsibly.** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "result = llm.invoke(\"What is an LLM?\")\n",
    "\n",
    "print(result.__dict__.keys())\n",
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gpt2 = gpt2.invoke(\"What is an LLM?\")\n",
    "Markdown(result_gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
      "\n",
      "filler context\n",
      "\n",
      "Question: filler question\n",
      "\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages\n",
    "\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Harmonized standards, whose references are published in the Official Journal of the European Union, are presumed to comply with the requirements of the regulation. This applies to both product requirements and system or process requirements, such as quality management systems. Notified bodies are required to assess conformity with harmonized standards when manufacturers use them. (Pages 16 and 197) \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        page_number = doc.metadata[\"page\"] + 1 \n",
    "        content_with_page = f\"Page {page_number}:\\n{doc.page_content}\"\n",
    "        formatted_docs.append(content_with_page)\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "Markdown(rag_chain.invoke(\"Describe the use of harmonised standards\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but the provided text does not contain any information about LLMs (Large Language Models). \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(rag_chain.invoke(\"What is an LLM?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838e5b3f98734552ad5b17be6dc813a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', continuous_update=False, description='Input:', layout=Layout(width='500px'), placeholder='Type …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c8d0dac7d142a2bff991f1a241d92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acc7a78a9a248a98b7d036c2a915a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Output(),), titles=('Complete generated prompt',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f3d15b2b794f209f38b7a357467aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Function that processes the user's question\n",
    "def process_input(user_input):\n",
    "    complete_prompt = f\"WIP\"\n",
    "    answer = rag_chain.invoke(user_input)\n",
    "    return answer, complete_prompt\n",
    "\n",
    "# Text input widget (for user question)\n",
    "text_input = widgets.Text(\n",
    "    description='Input:',\n",
    "    placeholder='Type something here...',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "# Primary output widget to display LLM's answer\n",
    "primary_output = widgets.Output()\n",
    "\n",
    "# Secondary output widget for the complete generated prompt (collapsible)\n",
    "secondary_output = widgets.Output()\n",
    "\n",
    "# Progress indicator (shown while processing)\n",
    "progress_indicator = widgets.Output()\n",
    "\n",
    "# Event handler for when Enter is pressed in the text input\n",
    "def on_text_submit(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        input_value = change.new.strip()\n",
    "        if input_value == \"\":\n",
    "            return\n",
    "\n",
    "        # Show the progress indicator\n",
    "        with progress_indicator:\n",
    "            progress_indicator.clear_output()\n",
    "            print(\"Processing... Please wait.\")\n",
    "\n",
    "        answer, complete_prompt = process_input(input_value)  # Processes the input\n",
    "\n",
    "        with primary_output:\n",
    "            primary_output.clear_output()       # Clears the previous output\n",
    "            print(f\"User Question: {input_value}\")\n",
    "            print(\"LLM Answer:\")\n",
    "            print(answer)\n",
    "\n",
    "        with secondary_output:\n",
    "            secondary_output.clear_output()     # Clears the previous output\n",
    "            print(\"Complete prompt:\")\n",
    "            print(complete_prompt)\n",
    "\n",
    "        # Hide the progress indicator after processing is complete\n",
    "        with progress_indicator:\n",
    "            progress_indicator.clear_output()\n",
    "\n",
    "        change.new = \"\"\n",
    "\n",
    "# Attach the event handler to the text input widget for Enter key submission\n",
    "text_input.continuous_update = False\n",
    "text_input.observe(on_text_submit, names='value', type=\"change\")\n",
    "\n",
    "# Make the complete generated prompt collapsible\n",
    "accordion = widgets.Accordion(children=[secondary_output])\n",
    "accordion.set_title(0, 'Complete generated prompt')\n",
    "\n",
    "# Display all the fields: text input, LLM's answer, complete prompt\n",
    "display(text_input, primary_output, accordion, progress_indicator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Harmonised standards are standards that are recognized and published by the Official Journal of the European Union (page 16). These standards are presumed to be in conformity with the requirements of the regulation covered by those standards (page 16). The notified body will assess conformity with those standards when a manufacturer uses them (page 197). \\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-ee53edca-2e80-4a8c-bd08-20eaef1570a8-0', usage_metadata={'input_tokens': 677, 'output_tokens': 69, 'total_tokens': 746, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a Q&A chatbot that helps to answer the user's questions about a given document. Always follow these rules to answer the question:\n",
    "\n",
    "Use the following pieces of context to answer the questions.\n",
    "If the question is not related to the context, just say it is not related.\n",
    "If you don't know the answer to any of the questions, just say that you don't know, don't try to make up an answer.\n",
    "Always mention in which pages the information you give are found.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_template,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        page_number = doc.metadata[\"page\"] + 1 \n",
    "        content_with_page = f\"Page {page_number}:\\n{doc.page_content}\"\n",
    "        formatted_docs.append(content_with_page)\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "docs = (retriever | format_docs).invoke(\"Describe the use of harmonised standards\")\n",
    "\n",
    "chain = question_answering_prompt | llm\n",
    "\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"context\" : docs,\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Describe the use of harmonised standards\"\n",
    "            ),\n",
    "        ],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "chat_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def QuestionAnswerLoop():\n",
    "    print(\"Enter your question (type 'quit' to exit): \")\n",
    "    while True:\n",
    "        user_input = input(\"Enter your question (type 'quit' to exit): \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Exiting Q&A chat. Goodbye!\")\n",
    "            break\n",
    "        else:\n",
    "            # get document blocks from retriever\n",
    "            retrieved_docs = (retriever | format_docs).invoke(user_input)\n",
    "            \n",
    "            # Add the user's question to the chat history\n",
    "            chat_history.add_user_message(user_input)\n",
    "\n",
    "            # Generate the response with context from the retrieved documents\n",
    "            response = chain.invoke(\n",
    "                {\n",
    "                    \"messages\": chat_history.messages,\n",
    "                    \"context\": retrieved_docs,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Add the AI's response to the chat history\n",
    "            chat_history.add_ai_message(response)\n",
    "\n",
    "            # Print the response\n",
    "            print(\"Question: \" + user_input)\n",
    "            print(\"Answer: \" + response.content)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what are some retrictions on medical devices in the EU?\n",
      "Answer: The document states that national laws concerning the organization, delivery, or financing of health services can impose restrictions on medical devices. For example, Page 5 mentions that certain devices might require a medical prescription, be dispensed only by specific professionals, or require professional counseling. \n",
      "\n",
      "Question: tell me more about the need for medical prescriptions\n",
      "Answer: This document does not provide specific information about the need for medical prescriptions for medical devices. \n",
      "\n",
      "Question: what does article 39 talk about?\n",
      "Answer: Article 39, found on Page 52, discusses the re-assessment of notified bodies in the EU.  It states that the authority responsible for notified bodies can conduct a complete re-assessment before the dates mentioned in the first subparagraph, either upon request by the notified body or if they have concerns about the body's continued compliance with the requirements. \n",
      "\n",
      "Question: what are \"notified bodies\"?\n",
      "Answer: Notified bodies are organizations designated by a Member State to perform conformity assessment activities for medical devices.  They are responsible for ensuring that medical devices meet the requirements of the EU's Medical Devices Regulation.  You can find more information about notified bodies on Page 50. \n",
      "\n",
      "Exiting Q&A chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "QuestionAnswerLoop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAAC_proj1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
