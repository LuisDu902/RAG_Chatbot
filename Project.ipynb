{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Chatbots for Technical Documentation\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Load and split the document](#load-and-split-the-document)\n",
    "- [Generate and store the embeddings](#generate-and-store-the-embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This project involves implementing a retrieval augmented generation (RAG) with `LangChain` to create a chatbot for\n",
    "answering questions about technical documentation. The document chosen for this assignment was the following: The European Union Medical Device Regulation - Regulation (EU) 2017/745 (EU MDR). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Install the packages and dependencies to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install langchain\n",
    "%pip install -qU langchain langchain-community langchain-chroma langchain-text-splitters unstructured sentence_transformers langchain-huggingface huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'document.html', 'start_index': 763}\n",
      "\n",
      "After consulting the Committee of the Regions,\n",
      "\n",
      "Acting in accordance with the ordinary legislative procedure (2),\n",
      "\n",
      "Whereas:\n",
      "\n",
      "(1) Council Directive 90/385/EEC ( 3 ) and Council Directive 93/42/EEC ( 4 ) constitute the Union regulatory framework for medical devices, other than in vitro diagnostic medical devices. However, a fundamental revision of those Directives is needed to establish a robust, transparent, predictable and sustainable regulatory framework for medical devices which ensures a high level of safety and health whilst supporting innovation.\n",
      "853\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "file_path = \"document.html\"\n",
    "\n",
    "loader = UnstructuredHTMLLoader(file_path)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "docs = loader.load_and_split(text_splitter)\n",
    "\n",
    "print(f\"{docs[1].metadata}\\n\")\n",
    "print(docs[1].page_content)\n",
    "print(len(docs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and store the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Athos\\anaconda3\\envs\\TAAC_proj1\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Generate and store the embeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Retrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first subparagraph shall also apply to system or process requirements to be fulfilled in accordance with this Regulation by economic operators or sponsors, including those relating to quality management systems, risk management, post-market surveillance systems, clinical investigations, clinical evaluation or post-market clinical follow-up (‘PMCF’).\n",
      "\n",
      "References in this Regulation to harmonised standards shall be understood as meaning harmonised standards the references of which have been published in the Official Journal of the European Union.\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"Describe the use harmonized standards\")\n",
    "\n",
    "len(retrieved_docs)\n",
    "\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Athos\\anaconda3\\envs\\TAAC_proj1\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\Athos\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "\n",
      "\n",
      "As you can see from the above picture, the first thing that happened was the World Cup. In 1994, the FIFA World Cup was held in Brazil. The first game was played in the city of Rio de Janeiro, and the second game was in the city of Sao Paulo. In the second game, the players were playing in the same stadium. The players were playing against each other, so the first game was the first game of the tournament.\n",
      "\n",
      "The second game was the first match of the tournament.\n",
      "\n",
      "The players were playing against each other, so the second game was the first match of the tournament.\n",
      "\n",
      "The third game was the second match of the tournament.\n",
      "\n",
      "The third game was the second match of the tournament.\n",
      "\n",
      "The fourth game was the fourth match of the tournament.\n",
      "\n",
      "The fifth game was the fifth match of the tournament.\n",
      "\n",
      "The sixth game was the sixth match of the tournament.\n",
      "\n",
      "The seventh game was the seventh match of the tournament.\n",
      "\n",
      "The eighth game was the eighth match of the tournament.\n",
      "\n",
      "The ninth game was the ninth match of the tournament.\n",
      "\n",
      "The tenth game was the tenth match of the tournament.\n",
      "\n",
      "The eleventh game was the eleventh match of the tournament.\n",
      "\n",
      "The twelfth game was the twelfth match of the tournament.\n",
      "\n",
      "The thirteenth game was the thirteenth match of the tournament.\n",
      "\n",
      "The thirteenth game was the thirteenth match of the tournament.\n",
      "\n",
      "The thirteenth game was the thirteenth match of the tournament.\n",
      "\n",
      "The thirteenth game was the thirteenth match of the tournament.\n",
      "\n",
      "The thirteenth game was the thirteenth match of the tournament.\n",
      "\n",
      "The thirteenth game was the thirteenth match of the tournament.\n",
      "\n",
      "The thirteenth game was the thirteenth match of the tournament.\n",
      "\n",
      "The thirteenth game was the thirteenth match of the tournament.\n",
      "\n",
      "The thirteenth game was the thirteenth match of the tournament.\n",
      "\n",
      "The thirteenth game was the thirteenth match of the tournament.\n",
      "\n",
      "The thirteenth game was the thirteenth match of the tournament.\n",
      "\n",
      "The thirteenth game was the thirteenth match of the tournament.\n",
      "\n",
      "The thirteenth game was the thirteenth match of the tournament.\n",
      "\n",
      "The thirteenth game was the thirteenth match of the tournament.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "repo_id = \"openai-community/gpt2\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id, max_length=128, temperature=0.5, token=HUGGINGFACEHUB_API_TOKEN\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "#from langchain import hub\n",
    "\n",
    "#prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages\n",
    "\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The use of harmonised standards is a key element in the development of new standards. The use of harmonised standards is not only a means to ensure that new standards are applied to the same standards, but also to ensure that they are applied to the same standards in the same way.\n",
      "\n",
      "The use of harmonised standards is a key element in the development of new standards. The use of harmonised standards is not only a means to ensure that new standards are applied to the same standards, but also to ensure that they are applied to the same standards in the same way.\n",
      "\n",
      "The use of harmonised standards is a key element in the development of new standards. The use of harmonised standards is not only a means to ensure that new standards are applied to the same standards, but also to ensure that they are applied to the same standards in the same way.\n",
      "\n",
      "The use of harmonised standards is a key element in the development of new standards. The use of harmonised standards is a means to ensure that new standards are applied to the same standards in the same way.\n",
      "\n",
      "The use of harmonised standards is a means to ensure that new standards are applied to the same standards in the same way.\n",
      "\n",
      "The use of harmonised standards is a means to ensure that new standards are applied to the same standards in the same way.\n",
      "\n",
      "The use of harmonised standards is a means to ensure that new standards are applied to the same standards in the same way.\n",
      "\n",
      "The use of harmonised standards is a means to ensure that new standards are applied to the same standards in the same way.\n",
      "\n",
      "The use of harmonised standards is a means to ensure that new standards are applied to the same standards in the same way.\n",
      "\n",
      "The use of harmonised standards is a means to ensure that new standards are applied to the same standards in the same way.\n",
      "\n",
      "The use of harmonised standards is a means to ensure that new standards are applied to the same standards in the same way.\n",
      "\n",
      "The use of harmonised standards is a means to ensure that new standards are applied to the same standards in the same way.\n",
      "\n",
      "The use of harmonised standards is a means to ensure that new standards are applied to the same standards in the same way.\n",
      "\n",
      "The use of harmonised standards is a means to ensure that new standards are applied to the same standards in the same way.\n",
      "\n",
      "The use of harmonised standards is a means to ensure that new standards are applied to the same standards in\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(rag_chain.invoke(\"Describe the use harmonized standards\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAAC_proj1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
