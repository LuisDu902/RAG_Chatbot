{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Chatbots for Technical Documentation\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Load and split the document](#load-and-split-the-document)\n",
    "- [Generate and store the embeddings](#generate-and-store-the-embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This project involves implementing a retrieval augmented generation (RAG) with `LangChain` to create a chatbot for\n",
    "answering questions about technical documentation. The document chosen for this assignment was the following: The European Union Medical Device Regulation - Regulation (EU) 2017/745 (EU MDR). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Install the packages and dependencies to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install -qU langchain langchain-community langchain-chroma langchain-text-splitters unstructured sentence_transformers langchain-huggingface huggingface_hub pdfplumber langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='02017R0745 — EN — 09.07.2024 — 004.001 — 1\n",
      "This text is meant purely as a documentation tool and has no legal effect. The Union's institutions do not assume any liability\n",
      "for its contents. The authentic versions of the relevant acts, including their preambles, are those published in the Official\n",
      "Journal of the European Union and available in EUR-Lex. Those official texts are directly accessible through the links\n",
      "embedded in this document\n",
      "►B REGULATION (EU) 2017/745 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\n",
      "of 5 April 2017\n",
      "on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and\n",
      "Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC\n",
      "(Text with EEA relevance)\n",
      "(OJ L 117, 5.5.2017, p. 1)\n",
      "Amended by:\n",
      "Official Journal\n",
      "No page date\n",
      "►M1 Regulation (EU) 2020/561 of the European Parliament and of the L 130 18 24.4.2020\n",
      "Council of 23 April 2020\n",
      "►M2 Commission Delegated Regulation (EU) 2023/502 of 1 December 2022 L 70 1 8.3.2023' metadata={'source': 'document.pdf', 'file_path': 'document.pdf', 'page': 0, 'total_pages': 232, 'ModDate': \"D:20240808025522+02'00'\", 'Producer': '3-Heights(TM) PDF to PDF-A Converter Shell 4.7.24.2 (http://www.pdf-tools.com)', 'Title': 'CL2017R0745EN0040010.0001.3bi_cp 1..1', 'Author': 'Publications Office', 'Subject': ' ', 'Creator': 'Arbortext Advanced Print Publisher 10.0.1465/W Unicode', 'CreationDate': \"D:20240724041003-07'00'\", 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "# Using PDF document\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "loader = PDFPlumberLoader(\"document.pdf\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "pages = loader.load_and_split(text_splitter)\n",
    "\n",
    "print(pages[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and store the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and store the embeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", task_type=\"retrieval_document\")\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=pages, embedding=embeddings, persist_directory=\"db\")\n",
    "# vectorstore = Chroma(persist_directory=\"db\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Retrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 169: of those staff, in order to ensure that personnel who carry out and perform\n",
      "assessment and verification operations are competent to fulfil the tasks\n",
      "required of them.\n",
      "page 127: ated with each hazard as well as the overall residual risk is judged\n",
      "acceptable. In selecting the most appropriate solutions, manufacturers\n",
      "shall, in the following order of priority:\n",
      "(a) eliminate or reduce risks as far as possible through safe design and\n",
      "manufacture;\n",
      "page 196: conformity assessment procedures,\n",
      "— identification of applicable general safety and performance\n",
      "requirements and solutions to fulfil those requirements, taking\n",
      "applicable CS and, where opted for, harmonised standards or\n",
      "other adequate solutions into account,\n",
      "— risk management as referred to in Secti\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"Describe the use of harmonised standards\")\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(\"page \" + str(doc.metadata[\"page\"] + 1) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ':\\n\\nThis is an extremely important question—how can a large proportion of people who may not speak the'},\n",
       " {'generated_text': ' to encourage good use of the same product\\n\\n(2)Where there is a strong preference among experts in'},\n",
       " {'generated_text': \", including the EU's 'Duke of Limbo' regulations as a 'guarantee that the countries\"},\n",
       " {'generated_text': ', one based on international practice of the EU.\\n\\n1. A national harmonised standard: whether under'},\n",
       " {'generated_text': '. I find that most of the information I have for a particular type of document has nothing to do with the'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2', max_length=1000, pad_token_id=50256, return_full_text=False)\n",
    "\n",
    "gpt2 = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "set_seed(42)\n",
    "generator(\"Describe the use of harmonised standards\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['content', 'additional_kwargs', 'response_metadata', 'type', 'name', 'id', 'example', 'tool_calls', 'invalid_tool_calls', 'usage_metadata'])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "LLM stands for **Large Language Model**.  It's a type of artificial intelligence (AI) that's trained on a massive dataset of text and code. This allows LLMs to understand and generate human-like text, making them incredibly versatile and powerful.\n",
       "\n",
       "Here's a breakdown of what makes an LLM:\n",
       "\n",
       "**Key Characteristics:**\n",
       "\n",
       "* **Vast Training Data:** LLMs are trained on massive datasets of text and code, often scraped from the internet or curated from specific sources. This gives them a broad understanding of language and context.\n",
       "* **Deep Learning Architecture:** LLMs use deep learning techniques, specifically neural networks, to learn complex patterns and relationships within the training data. This allows them to generate coherent and contextually relevant text.\n",
       "* **Natural Language Processing (NLP):** LLMs are fundamentally NLP models, meaning they are designed to understand and interact with human language. They can perform tasks like:\n",
       "    * **Text generation:** Writing stories, poems, articles, emails, etc.\n",
       "    * **Translation:** Translating text between languages.\n",
       "    * **Summarization:** Condensing large amounts of text into concise summaries.\n",
       "    * **Question answering:** Answering questions based on provided text.\n",
       "    * **Code generation:** Generating code in various programming languages.\n",
       "\n",
       "**Popular Examples:**\n",
       "\n",
       "* **GPT-3 (Generative Pre-trained Transformer 3)** by OpenAI\n",
       "* **LaMDA (Language Model for Dialogue Applications)** by Google\n",
       "* **BERT (Bidirectional Encoder Representations from Transformers)** by Google\n",
       "* **PaLM (Pathways Language Model)** by Google\n",
       "\n",
       "**Applications:**\n",
       "\n",
       "LLMs have a wide range of applications, including:\n",
       "\n",
       "* **Chatbots and conversational AI:** Creating more natural and engaging chatbots.\n",
       "* **Content creation:** Generating articles, blog posts, social media content, and more.\n",
       "* **Education and research:** Helping students learn and researchers analyze data.\n",
       "* **Customer service:** Automating customer support tasks.\n",
       "* **Software development:** Generating code and automating coding tasks.\n",
       "\n",
       "**Important Considerations:**\n",
       "\n",
       "* **Bias:** LLMs can reflect biases present in their training data, which can lead to problematic outputs.\n",
       "* **Ethical concerns:** There are concerns about LLMs being used for malicious purposes, such as generating fake news or impersonating individuals.\n",
       "* **Transparency:** Understanding how LLMs work and making their decision-making processes more transparent is crucial.\n",
       "\n",
       "LLMs are a rapidly evolving field, and their potential applications are constantly expanding. As they continue to improve, they will likely play an increasingly important role in our lives.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "result = llm.invoke(\"What is an LLM?\")\n",
    "\n",
    "print(result.__dict__.keys())\n",
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "To be precise, a LLM should be a type that defines a set of parameters. The first argument to an implementation of an LLM is the form of the argument. The second argument is the argument's definition, the final form of the argument, and so on. These are called a \"argument semantics\". These semantics are also often referred to as the \"argument model\". It's the same concept as for types, and will hold even with some generalizations based on LLM's semantics. LLM's semantics are implemented in a number of different ways, and some examples are given in the following section.\n",
       "\n",
       "Basic LLMs\n",
       "\n",
       "First, make sure that the argument's definition has already been applied at initialization, otherwise LLM will call the type again without an argument in place. However, sometimes this can occur. For example, when an integer type is passed to a function, the type of the method returned is not guaranteed to come from a valid function. In this case you are forced to convert the name of the method to a valid function call. Alternatively, in some cases, you might omit the name of the method, so that the resulting type will resemble the class definition.\n",
       "\n",
       "Next, simply write the definition into the arguments, and then write the same for each argument as indicated in the LLM syntax.\n",
       "\n",
       "This is a few examples. The final form may be a list. Note, that every argument declaration has a certain format.\n",
       "\n",
       "If you include this version of a definition in your documentation, the compiler ignores all occurrences of the final version of the definition. This could cause you to break the grammar.\n",
       "\n",
       "If you specify you believe that this example is the first time a function is provided, this is the first time you'll ever use the \"f\" (for --) notation to evaluate a call in the definition.\n",
       "\n",
       "Other types may need to be specified and may also need to be called separately. This is where two values are taken off-line, and the \"name\" of the other is also taken off-line.\n",
       "\n",
       "The \"type\" of the rest of an input argument is usually taken from the form of a name of the type. In this example, the name of the type is not taken into account.\n",
       "\n",
       "The actual name of the variable, as defined, is given as a single list of \"types\".\n",
       "\n",
       "To call a method, you also need to specify a specific argument; typically what the method calls, the call context, and all the parameters you call it with. For example, the following might be interpreted as follows:\n",
       "\n",
       "return {... }\n",
       "\n",
       "This \"is\" option will call a method that gets a value returned.\n",
       "\n",
       "For more examples of how to write an LLM, see LLM vs. Type. In the next section, I'll present the problem with using the Type model and other classes (called \"class constructors\", or \"interface classes\") in class evaluation, in contrast to classes which are described below.\n",
       "\n",
       "The LLM model is essentially the idea behind all of this documentation. This particular article will take a look at the LLM model described below by the name of a variable, the class, and the name of the methods and arguments.\n",
       "\n",
       "LLLM Models\n",
       "\n",
       "LLLM also refers to other types. For example, here is the LLM syntax for a method on some object.\n",
       "\n",
       "type F = f a b... object F.prototype : (f: F) -> f... F.prototype : (f: F) -> f (object F)\n",
       "\n",
       "The name, type, and constructor parameters may be assigned when defining an instance(or the object instance if not a constructor or an instanceclass). This is useful for defining generic methods to be used in an application.\n",
       "\n",
       "This way: an instance function can be passed as an argument. If an instance variable is passed to f(foo), the given function will be called by doing something like this:\n",
       "\n",
       "let foo = new Foo // foo is a constructor. let f a = new Foo // foo is a supertype (class F);\n",
       "\n",
       "For example, you may want to take a copy argument to f(foo); and define the following:\n",
       "\n",
       "let f = new Foo // foo is a supertype (class F);\n",
       "\n",
       "The argument(s) function as indicated by the type is not only called by f(foo)(), but also by (f f a ) which is then called by doing nothing like this:\n",
       "\n",
       "let f = new Foo // foo is in a constructor. let f a = new Foo // foo is a supertype (class F);\n",
       "\n",
       "The constructor, f'. This method calls an object instance of F and prints out the following:\n",
       "\n",
       "let f = new Foo // foo is"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_gpt2 = gpt2.invoke(\"What is an LLM?\")\n",
    "Markdown(result_gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
      "\n",
      "filler context\n",
      "\n",
      "Question: filler question\n",
      "\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages\n",
    "\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Harmonised standards are used to identify applicable general safety and performance requirements and solutions to fulfill those requirements.  This is mentioned in the context of conformity assessment procedures on page 196. They are considered alongside other adequate solutions, such as applicable CS standards. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def format_docs(docs):\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        page_number = doc.metadata[\"page\"] + 1 \n",
    "        content_with_page = f\"Page {page_number}:\\n{doc.page_content}\"\n",
    "        formatted_docs.append(content_with_page)\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    ")\n",
    "\n",
    "query = \"Describe the use of harmonised standards\"\n",
    "Markdown(rag_chain.invoke(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Describe the use of harmonised standards',\n",
       " 'result': 'Harmonised standards are used to identify applicable general safety and performance requirements and solutions to fulfil those requirements. This is mentioned in the conformity assessment procedures, specifically in the identification of applicable general safety and performance requirements and solutions. This information is found on page 15. \\n',\n",
       " 'source_documents': [Document(metadata={'Author': 'Publications Office', 'CreationDate': \"D:20240724041003-07'00'\", 'Creator': 'Arbortext Advanced Print Publisher 10.0.1465/W Unicode', 'ModDate': \"D:20240808025522+02'00'\", 'Producer': '3-Heights(TM) PDF to PDF-A Converter Shell 4.7.24.2 (http://www.pdf-tools.com)', 'Subject': ' ', 'Title': 'CL2017R0745EN0040010.0001.3bi_cp 1..1', 'file_path': 'document.pdf', 'page': 168, 'source': 'document.pdf', 'start_index': 3213, 'total_pages': 232}, page_content='of those staff, in order to ensure that personnel who carry out and perform\\nassessment and verification operations are competent to fulfil the tasks\\nrequired of them.'),\n",
       "  Document(metadata={'Author': 'Publications Office', 'CreationDate': \"D:20240724041003-07'00'\", 'Creator': 'Arbortext Advanced Print Publisher 10.0.1465/W Unicode', 'ModDate': \"D:20240808025522+02'00'\", 'Producer': '3-Heights(TM) PDF to PDF-A Converter Shell 4.7.24.2 (http://www.pdf-tools.com)', 'Subject': ' ', 'Title': 'CL2017R0745EN0040010.0001.3bi_cp 1..1', 'file_path': 'document.pdf', 'page': 126, 'source': 'document.pdf', 'start_index': 2409, 'total_pages': 232}, page_content='ated with each hazard as well as the overall residual risk is judged\\nacceptable. In selecting the most appropriate solutions, manufacturers\\nshall, in the following order of priority:\\n(a) eliminate or reduce risks as far as possible through safe design and\\nmanufacture;'),\n",
       "  Document(metadata={'Author': 'Publications Office', 'CreationDate': \"D:20240724041003-07'00'\", 'Creator': 'Arbortext Advanced Print Publisher 10.0.1465/W Unicode', 'ModDate': \"D:20240808025522+02'00'\", 'Producer': '3-Heights(TM) PDF to PDF-A Converter Shell 4.7.24.2 (http://www.pdf-tools.com)', 'Subject': ' ', 'Title': 'CL2017R0745EN0040010.0001.3bi_cp 1..1', 'file_path': 'document.pdf', 'page': 195, 'source': 'document.pdf', 'start_index': 2371, 'total_pages': 232}, page_content='conformity assessment procedures,\\n— identification of applicable general safety and performance\\nrequirements and solutions to fulfil those requirements, taking\\napplicable CS and, where opted for, harmonised standards or\\nother adequate solutions into account,\\n— risk management as referred to in Section 3 of Annex I,\\n— the clinical evaluation, pursuant to Article 61 and Annex XIV,\\nincluding post-market clinical follow-up,')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = qa_chain.invoke({\"query\": query})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but the provided context does not contain any information about LLMs (Large Language Models). Therefore, I cannot answer your question. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(rag_chain.invoke(\"What is an LLM?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c5213f42fe4283b48ed995e57014a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', continuous_update=False, description='Input:', layout=Layout(width='500px'), placeholder='Type …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67f97b54a9048588aa609a4e0c73c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8915343750a848fd808161ad50cdfd00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Output(),), titles=('Complete generated prompt',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bef338b1a046999f40e76581f4010d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Function that processes the user's question\n",
    "def process_input(user_input):\n",
    "    complete_prompt = f\"WIP\"\n",
    "    answer = rag_chain.invoke(user_input)\n",
    "    return answer, complete_prompt\n",
    "\n",
    "# Text input widget (for user question)\n",
    "text_input = widgets.Text(\n",
    "    description='Input:',\n",
    "    placeholder='Type something here...',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "# Primary output widget to display LLM's answer\n",
    "primary_output = widgets.Output()\n",
    "\n",
    "# Secondary output widget for the complete generated prompt (collapsible)\n",
    "secondary_output = widgets.Output()\n",
    "\n",
    "# Progress indicator (shown while processing)\n",
    "progress_indicator = widgets.Output()\n",
    "\n",
    "# Event handler for when Enter is pressed in the text input\n",
    "def on_text_submit(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        input_value = change.new.strip()\n",
    "        if input_value == \"\":\n",
    "            return\n",
    "\n",
    "        # Show the progress indicator\n",
    "        with progress_indicator:\n",
    "            progress_indicator.clear_output()\n",
    "            print(\"Processing... Please wait.\")\n",
    "\n",
    "        answer, complete_prompt = process_input(input_value)  # Processes the input\n",
    "\n",
    "        with primary_output:\n",
    "            primary_output.clear_output()       # Clears the previous output\n",
    "            print(f\"User Question: {input_value}\")\n",
    "            print(\"LLM Answer:\")\n",
    "            print(answer)\n",
    "\n",
    "        with secondary_output:\n",
    "            secondary_output.clear_output()     # Clears the previous output\n",
    "            print(\"Complete prompt:\")\n",
    "            print(complete_prompt)\n",
    "\n",
    "        # Hide the progress indicator after processing is complete\n",
    "        with progress_indicator:\n",
    "            progress_indicator.clear_output()\n",
    "\n",
    "        change.new = \"\"\n",
    "\n",
    "# Attach the event handler to the text input widget for Enter key submission\n",
    "text_input.continuous_update = False\n",
    "text_input.observe(on_text_submit, names='value', type=\"change\")\n",
    "\n",
    "# Make the complete generated prompt collapsible\n",
    "accordion = widgets.Accordion(children=[secondary_output])\n",
    "accordion.set_title(0, 'Complete generated prompt')\n",
    "\n",
    "# Display all the fields: text input, LLM's answer, complete prompt\n",
    "display(text_input, primary_output, accordion, progress_indicator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
