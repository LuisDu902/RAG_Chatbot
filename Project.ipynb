{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Chatbots for Technical Documentation\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Load and split the document](#load-and-split-the-document)\n",
    "- [Generate and store the embeddings](#generate-and-store-the-embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This project involves implementing a retrieval augmented generation (RAG) with `LangChain` to create a chatbot for\n",
    "answering questions about technical documentation. The document chosen for this assignment was the following: The European Union Medical Device Regulation - Regulation (EU) 2017/745 (EU MDR). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Install the packages and dependencies to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install -qU langchain langchain-community langchain-chroma langchain-text-splitters unstructured sentence_transformers langchain-huggingface huggingface_hub pdfplumber langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='02017R0745 — EN — 09.07.2024 — 004.001 — 1\n",
      "This text is meant purely as a documentation tool and has no legal effect. The Union's institutions do not assume any liability\n",
      "for its contents. The authentic versions of the relevant acts, including their preambles, are those published in the Official\n",
      "Journal of the European Union and available in EUR-Lex. Those official texts are directly accessible through the links\n",
      "embedded in this document\n",
      "►B REGULATION (EU) 2017/745 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\n",
      "of 5 April 2017\n",
      "on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and\n",
      "Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC\n",
      "(Text with EEA relevance)\n",
      "(OJ L 117, 5.5.2017, p. 1)\n",
      "Amended by:\n",
      "Official Journal\n",
      "No page date\n",
      "►M1 Regulation (EU) 2020/561 of the European Parliament and of the L 130 18 24.4.2020\n",
      "Council of 23 April 2020\n",
      "►M2 Commission Delegated Regulation (EU) 2023/502 of 1 December 2022 L 70 1 8.3.2023' metadata={'source': 'document.pdf', 'file_path': 'document.pdf', 'page': 0, 'total_pages': 232, 'ModDate': \"D:20240808025522+02'00'\", 'Producer': '3-Heights(TM) PDF to PDF-A Converter Shell 4.7.24.2 (http://www.pdf-tools.com)', 'Title': 'CL2017R0745EN0040010.0001.3bi_cp 1..1', 'Author': 'Publications Office', 'Subject': ' ', 'Creator': 'Arbortext Advanced Print Publisher 10.0.1465/W Unicode', 'CreationDate': \"D:20240724041003-07'00'\", 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "# Using PDF document\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "loader = PDFPlumberLoader(\"document.pdf\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "pages = loader.load_and_split(text_splitter)\n",
    "\n",
    "print(pages[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and store the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and store the embeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# vectorstore = Chroma.from_documents(documents=pages, embedding=embeddings, persist_directory=\"db\")\n",
    "vectorstore = Chroma(persist_directory=\"db\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Retrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 16: which have been published in the Official Journal of the European\n",
      "Union, shall be presumed to be in conformity with the requirements\n",
      "of this Regulation covered by those standards or parts thereof.\n",
      "The first subparagraph shall also apply to system or process\n",
      "requirements to be fulfilled in accordance\n",
      "page 197: used, particularly as regards sterilisation and the relevant documents;\n",
      "and\n",
      "(e) the appropriate tests and trials which are to be carried out before,\n",
      "during and after manufacture, the frequency with which they are to\n",
      "take place, and the test equipment to be used; it shall be possible to\n",
      "trace back ad\n",
      "page 168: an initial start-up phase.\n",
      "1.6. Participation in coordination activities\n",
      "1.6.1. The notified body shall participate in, or ensure that its assessment\n",
      "personnel is informed of, any relevant standardisation activities and in\n",
      "the activities of the notified body coordination group referred to in\n",
      "Article\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"Describe the use of harmonised standards\")\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(\"page \" + str(doc.metadata[\"page\"] + 1) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ':\\n\\nThis is an extremely important question—how can a large proportion of people who may not speak the'},\n",
       " {'generated_text': ' to encourage good use of the same product\\n\\n(2)Where there is a strong preference among experts in'},\n",
       " {'generated_text': \", including the EU's 'Duke of Limbo' regulations as a 'guarantee that the countries\"},\n",
       " {'generated_text': ', one based on international practice of the EU.\\n\\n1. A national harmonised standard: whether under'},\n",
       " {'generated_text': '. I find that most of the information I have for a particular type of document has nothing to do with the'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2', max_length=1000, pad_token_id=50256, return_full_text=False)\n",
    "\n",
    "gpt2 = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "set_seed(42)\n",
    "generator(\"Describe the use of harmonised standards\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['content', 'additional_kwargs', 'response_metadata', 'type', 'name', 'id', 'example', 'tool_calls', 'invalid_tool_calls', 'usage_metadata'])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "LLM stands for **Large Language Model**. It's a type of artificial intelligence (AI) that excels at understanding and generating human-like text. \n",
       "\n",
       "Here's a breakdown:\n",
       "\n",
       "**What it is:**\n",
       "\n",
       "* **Neural Network:** LLMs are built on deep learning, specifically using a type of neural network called a **transformer**. This network learns patterns and relationships in vast amounts of text data.\n",
       "* **Massive Data Training:** They are trained on massive datasets of text, allowing them to acquire a broad understanding of language and its nuances.\n",
       "* **Text Generation:** They can generate coherent and contextually relevant text, such as articles, stories, poems, code, and more.\n",
       "* **Understanding and Responding:** They can understand the meaning of text and respond in a way that mimics human conversation.\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "* **Natural Language Processing (NLP):** LLMs are a powerful tool for NLP tasks like:\n",
       "    * **Text summarization:** Condensing large amounts of text into concise summaries.\n",
       "    * **Machine translation:** Translating text from one language to another.\n",
       "    * **Question answering:** Providing answers to questions based on given text.\n",
       "    * **Sentiment analysis:** Determining the emotional tone of text.\n",
       "* **Text Generation:** LLMs can:\n",
       "    * **Create creative content:** Write stories, poems, scripts, and more.\n",
       "    * **Generate code:** Write code in various programming languages.\n",
       "    * **Compose emails and letters:**  Draft professional and personal communications.\n",
       "\n",
       "**Examples:**\n",
       "\n",
       "* **GPT-3 (Generative Pre-trained Transformer 3):** Developed by OpenAI, it's one of the most well-known and powerful LLMs.\n",
       "* **LaMDA (Language Model for Dialogue Applications):** Google's conversational AI model used in Google Assistant.\n",
       "* **BERT (Bidirectional Encoder Representations from Transformers):** Developed by Google AI, it excels at understanding the meaning of text and is used in various NLP applications.\n",
       "\n",
       "**Applications:**\n",
       "\n",
       "* **Chatbots and virtual assistants:** Providing conversational AI experiences.\n",
       "* **Content creation:** Generating articles, marketing materials, and social media posts.\n",
       "* **Education:** Assisting students with writing, research, and learning.\n",
       "* **Customer service:** Automating responses and providing personalized support.\n",
       "* **Research and development:** Analyzing data, conducting research, and making predictions.\n",
       "\n",
       "**Limitations:**\n",
       "\n",
       "* **Bias and misinformation:** LLMs can reflect biases present in their training data, leading to inaccurate or harmful outputs.\n",
       "* **Lack of real-world understanding:** They lack common sense and understanding of the physical world.\n",
       "* **Ethical considerations:** LLMs raise concerns about the misuse of their capabilities, such as creating fake news or manipulating public opinion.\n",
       "\n",
       "**In conclusion,** LLMs are a rapidly evolving field with immense potential to revolutionize various aspects of our lives. However, it's crucial to use them responsibly and address their limitations to ensure their ethical and beneficial application.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = getpass()    # https://ai.google.dev/pricing\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "result = llm.invoke(\"What is an LLM?\")\n",
    "\n",
    "print(result.__dict__.keys())\n",
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gpt2 = gpt2.invoke(\"What is an LLM?\")\n",
    "Markdown(result_gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
      "\n",
      "filler context\n",
      "\n",
      "Question: filler question\n",
      "\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages\n",
    "\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Harmonised standards are used to demonstrate conformity with the requirements of the Regulation. If a manufacturer uses a harmonised standard, the notified body will assume conformity with those requirements, unless proven otherwise. This is stated on page 16 and page 197. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        page_number = doc.metadata[\"page\"] + 1 \n",
    "        content_with_page = f\"Page {page_number}:\\n{doc.page_content}\"\n",
    "        formatted_docs.append(content_with_page)\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "Markdown(rag_chain.invoke(\"Describe the use of harmonised standards\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but the provided context does not contain any information about an LLM. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(rag_chain.invoke(\"What is an LLM?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838e5b3f98734552ad5b17be6dc813a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', continuous_update=False, description='Input:', layout=Layout(width='500px'), placeholder='Type …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c8d0dac7d142a2bff991f1a241d92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acc7a78a9a248a98b7d036c2a915a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Output(),), titles=('Complete generated prompt',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f3d15b2b794f209f38b7a357467aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Function that processes the user's question\n",
    "def process_input(user_input):\n",
    "    complete_prompt = f\"WIP\"\n",
    "    answer = rag_chain.invoke(user_input)\n",
    "    return answer, complete_prompt\n",
    "\n",
    "# Text input widget (for user question)\n",
    "text_input = widgets.Text(\n",
    "    description='Input:',\n",
    "    placeholder='Type something here...',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "# Primary output widget to display LLM's answer\n",
    "primary_output = widgets.Output()\n",
    "\n",
    "# Secondary output widget for the complete generated prompt (collapsible)\n",
    "secondary_output = widgets.Output()\n",
    "\n",
    "# Progress indicator (shown while processing)\n",
    "progress_indicator = widgets.Output()\n",
    "\n",
    "# Event handler for when Enter is pressed in the text input\n",
    "def on_text_submit(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        input_value = change.new.strip()\n",
    "        if input_value == \"\":\n",
    "            return\n",
    "\n",
    "        # Show the progress indicator\n",
    "        with progress_indicator:\n",
    "            progress_indicator.clear_output()\n",
    "            print(\"Processing... Please wait.\")\n",
    "\n",
    "        answer, complete_prompt = process_input(input_value)  # Processes the input\n",
    "\n",
    "        with primary_output:\n",
    "            primary_output.clear_output()       # Clears the previous output\n",
    "            print(f\"User Question: {input_value}\")\n",
    "            print(\"LLM Answer:\")\n",
    "            print(answer)\n",
    "\n",
    "        with secondary_output:\n",
    "            secondary_output.clear_output()     # Clears the previous output\n",
    "            print(\"Complete prompt:\")\n",
    "            print(complete_prompt)\n",
    "\n",
    "        # Hide the progress indicator after processing is complete\n",
    "        with progress_indicator:\n",
    "            progress_indicator.clear_output()\n",
    "\n",
    "        change.new = \"\"\n",
    "\n",
    "# Attach the event handler to the text input widget for Enter key submission\n",
    "text_input.continuous_update = False\n",
    "text_input.observe(on_text_submit, names='value', type=\"change\")\n",
    "\n",
    "# Make the complete generated prompt collapsible\n",
    "accordion = widgets.Accordion(children=[secondary_output])\n",
    "accordion.set_title(0, 'Complete generated prompt')\n",
    "\n",
    "# Display all the fields: text input, LLM's answer, complete prompt\n",
    "display(text_input, primary_output, accordion, progress_indicator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
