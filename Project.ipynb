{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Chatbots for Technical Documentation\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Indexing](#indexing)\n",
    "- [Retriever](#retriever)\n",
    "- [Prompt](#prompt)\n",
    "- [LLM](#llm)\n",
    "- [RAG Chain](#rag-chain)\n",
    "- [Comparisons](#evaluation-metrics-and-comparison)\n",
    "- [Chat history](#chat-history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This project involves implementing a retrieval augmented generation (RAG) with *LangChain* to create a chatbot for\n",
    "answering questions about technical documentation. The document chosen for this assignment was the following: **The European Union Medical Device Regulation - Regulation (EU) 2017/745 (EU MDR)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Install the packages and dependencies to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install -qU langchain langchain-community langchain-chroma langchain-text-splitters unstructured sentence_transformers langchain-huggingface huggingface_hub pdfplumber langchain-google-genai ipywidgets python-dotenv lark chainlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Google's generative AI model is being used, ensure that the ``GOOGLE_API_KEY`` is securely stored in the ``.env`` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Firstly, we start by connecting to Google's generative AI embeddings model. The **Text Embeddings 004** model from Gemini is employed for the embedding generation, with the task_type set to *retrieval_document* to optimize embeddings for retrieval tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", task_type=\"retrieval_document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the indexing stage, we start by loading the PDF document and splitting it into manageable sections. To optimize execution time and improve efficiency, we store the vector store locally in a folder named \"db.\" This allows us to quickly access previously processed data without having to re-index the document each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "if os.path.exists(\"db\"):\n",
    "    vectorstore = Chroma(persist_directory=\"db\", embedding_function=embeddings)\n",
    "else:\n",
    "    loader = PDFPlumberLoader(\"document.pdf\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "    pages = loader.load_and_split(text_splitter)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=pages, embedding=embeddings, persist_directory=\"db\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever\n",
    "\n",
    "From the vector store, a retriever is created, configured to perform similarity searches and return the top 5 most relevant results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved document number : 5\n",
      "page 169: of those staff, in order to ensure that personnel who carry out and perform\n",
      "assessment and verification operations are competent to fulfil the tasks\n",
      "required of them.\n",
      "page 196: conformity assessment procedures,\n",
      "— identification of applicable general safety and performance\n",
      "requirements and solutions to fulfil those requirements, taking\n",
      "applicable CS and, where opted for, harmonised standards or\n",
      "other adequate solutions into account,\n",
      "— risk management as referred to in Secti\n",
      "page 147: purpose, and shall include a justification, validation and verification of the\n",
      "solutions adopted to meet those requirements. The demonstration of\n",
      "conformity shall include:\n",
      "(a) the general safety and performance requirements that apply to the device\n",
      "and an explanation as to why others do not apply;\n",
      "(\n",
      "page 179: — carry out the appropriate examinations and tests in order to verify that\n",
      "the solutions adopted by the manufacturer meet the general safety and\n",
      "performance requirements set out in Annex I. Such examinations and\n",
      "tests shall include all tests necessary to verify that the manufacturer\n",
      "has in fact appl\n",
      "page 153: description of the conformity assessment procedure performed and identifi­\n",
      "cation of the certificate or certificates issued;\n",
      "9. Where applicable, additional information;\n",
      "10. Place and date of issue of the declaration, name and function of the person\n",
      "who signed it as well as an indication for, and on\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"Describe the use of harmonised standards\")\n",
    "\n",
    "print(\"Retrieved document number : \" + str(len(retrieved_docs)))\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(\"page \" + str(doc.metadata[\"page\"] + 1) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "We establish a structured format for the prompts sent to the LLM. This prompt format conveys the context while instructing the LLM to refrain from answering when it lacks confidence, thereby minimizing the risk of hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
      "\n",
      "Context: filler context\n",
      "\n",
      "Question: filler question\n",
      "\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages\n",
    "\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM utilized in this project is **Gemini 1.5 Flash**, recognized as Google Gemini’s fastest multimodal model. It boasts an impressive context window of 1 million tokens, allowing for comprehensive understanding and processing of extensive inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['content', 'additional_kwargs', 'response_metadata', 'type', 'name', 'id', 'example', 'tool_calls', 'invalid_tool_calls', 'usage_metadata'])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "LLM stands for **Large Language Model**. It's a type of artificial intelligence (AI) that excels at understanding and generating human-like text. \n",
       "\n",
       "Here's a breakdown:\n",
       "\n",
       "**What is it?**\n",
       "\n",
       "* **Deep learning model:** An LLM is a type of neural network, a complex mathematical structure inspired by the human brain. It's trained on massive datasets of text and code.\n",
       "* **Text-based:** LLMs specialize in processing and generating textual information.\n",
       "* **Generative:** They can create new text content, not just analyze existing text.\n",
       "\n",
       "**How does it work?**\n",
       "\n",
       "* **Training:** LLMs are trained on vast amounts of text data, learning patterns, relationships, and nuances of language. This allows them to understand context, grammar, and meaning.\n",
       "* **Processing:** When you input text, the LLM analyzes the input and predicts the most likely next word or phrase based on its training data.\n",
       "* **Output:** The LLM generates coherent and contextually relevant text, often mimicking human writing style.\n",
       "\n",
       "**Examples of LLMs:**\n",
       "\n",
       "* **GPT-3 (Generative Pre-trained Transformer 3):** Developed by OpenAI, known for its ability to write different kinds of creative content, translate languages, and answer your questions in an informative way.\n",
       "* **LaMDA (Language Model for Dialogue Applications):** Developed by Google, designed for conversational AI, capable of engaging in open-ended, natural-sounding dialogue.\n",
       "* **BERT (Bidirectional Encoder Representations from Transformers):** Developed by Google, excels at understanding the meaning of words in context, enabling it to perform various tasks like question answering and sentiment analysis.\n",
       "\n",
       "**Applications of LLMs:**\n",
       "\n",
       "* **Chatbots and virtual assistants:**  Providing engaging and informative conversations.\n",
       "* **Content creation:** Generating articles, stories, poems, scripts, and more.\n",
       "* **Translation:** Translating text between languages accurately and naturally.\n",
       "* **Code generation:** Writing and debugging code in various programming languages.\n",
       "* **Summarization:** Condensing large amounts of text into concise summaries.\n",
       "* **Question answering:** Providing answers to questions based on a given text.\n",
       "\n",
       "**Key takeaway:** LLMs are powerful AI models that are revolutionizing how we interact with text and data. They offer a wide range of applications, from creating engaging content to automating tasks.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "result = llm.invoke(\"What is an LLM?\")\n",
    "\n",
    "print(result.__dict__.keys())\n",
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, we can now define a RAG chain that takes a question, retrieves relevant documents, constructs a prompt, passes it into a model, and parses the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        page_number = doc.metadata[\"page\"] + 1 \n",
    "        content_with_page = f\"Page {page_number}:\\n{doc.page_content}\"\n",
    "        formatted_docs.append(content_with_page)\n",
    "    return \"\\n\\n\".join(formatted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Medical Devices Regulation (MDR) is a European Union regulation that establishes requirements for the safety and performance of medical devices. It is found in Regulation (EU) 2017/746. The MDR aims to ensure that medical devices placed on the market in the EU meet high standards of safety and performance, and to protect public health. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the medical devices regulation?\"\n",
    "Markdown(rag_chain.invoke(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Temperature | Top P | Response |\n",
       "|-------------|-------|----------|\n",
       "| 0.1 | 0.1  | Harmonized standards are standards that have been published in the Official Journal of the European Union and are presumed to be in conformity with the requirements of the Regulation. This is found on page 16. \n",
       "| 0.1 | 0.5  | Harmonized standards are standards that have been published in the Official Journal of the European Union. These standards are presumed to be in conformity with the requirements of the Regulation. This information is found on page 16. \n",
       "| 0.1 | 0.9  | Harmonised standards are standards that have been published in the Official Journal of the European Union and are presumed to be in conformity with the requirements of the Regulation. This information can be found on page 16. \n",
       "| 0.5 | 0.1  | Harmonised standards are standards published in the Official Journal of the European Union that are presumed to be in conformity with the requirements of the Regulation. These standards cover system or process requirements for economic operators or sponsors, including quality management systems, risk management, and post-market surveillance systems.  This information is found on page 16 of the document. \n",
       "| 0.5 | 0.5  | Harmonised standards are standards that have been published in the Official Journal of the European Union and are presumed to be in conformity with the requirements of the Regulation. This is mentioned on page 16. \n",
       "| 0.5 | 0.9  | Harmonised standards are standards that have been published in the Official Journal of the European Union and are presumed to be in conformity with the requirements of the Regulation. These standards can be used to demonstrate conformity with general safety and performance requirements. This information is found on page 16. \n",
       "| 1.0 | 0.1  | Harmonised standards are standards that have been published in the Official Journal of the European Union. They are presumed to be in conformity with the requirements of the regulation. This information is found on page 16. \n",
       "| 1.0 | 0.5  | Harmonised standards are standards that have been published in the Official Journal of the European Union and are presumed to be in conformity with the requirements of the Regulation. This information is found on page 16. \n",
       "| 1.0 | 0.9  | Harmonised standards are standards that have been published in the Official Journal of the European Union. These standards are presumed to be in conformity with the requirements of the Regulation. (Page 16) \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures = [0.1, 0.5, 1.0]\n",
    "top_ps = [0.1, 0.5, 0.9]\n",
    "\n",
    "results = \"| Temperature | Top P | Response |\\n\" + \"|-------------|-------|----------|\\n\"\n",
    "\n",
    "for temperature in temperatures:\n",
    "    for top_p in top_ps:\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=temperature, top_p=top_p)\n",
    "\n",
    "        query = \"What is harmonised standards?\"\n",
    "        response = rag_chain.invoke(query)\n",
    "\n",
    "        results += f\"| {temperature} | {top_p}  | {response}\"\n",
    "\n",
    "Markdown(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini vs GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2', max_length=1000, pad_token_id=50256, return_full_text=False)\n",
    "gpt2 = HuggingFacePipeline(pipeline=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "The technical and legal definition provided by paragraph (e) of Article 2 shall be found in the 'Technical and Legal' annex\n",
       "\n",
       "of this Regulation or in the technical and legal annex to this Regulation.\n",
       "\n",
       "The following questions, which in themselves form an attempt to answer all questions raised here:\n",
       "\n",
       "Question 1:\n",
       "\n",
       "Who can request or receive a sample for testing purposes while holding a\n",
       "\n",
       "medical device for testing purposes?\n",
       "\n",
       "What constitutes a sample\n",
       "\n",
       "of the device's manufacturer's specifications?\n",
       "\n",
       "Question 2:\n",
       "\n",
       "Who can determine the quality from which\n",
       "\n",
       "the device is tested? What will\n",
       "\n",
       "the quality test be in fact?\n",
       "\n",
       "Question 3:\n",
       "\n",
       "Do not use to obtain or administer a device by\n",
       "\n",
       "in vitro testing or by a drug test that requires the\n",
       "\n",
       "treatment with any medicine whatsoever, even if to achieve\n",
       "\n",
       "this requirement, if it is not possible to test the device\n",
       "\n",
       "and if, by reason of this failure to comply with this restriction and the\n",
       "\n",
       "concern in the preceding paragraphs,\n",
       "\n",
       "It becomes necessary to obtain\n",
       "\n",
       "and administer samples"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain_gpt2 = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | gpt2\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"What is the medical devices regulation?\"\n",
    "Markdown(rag_chain_gpt2.invoke(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "simplified_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The provided text snippets don't contain information about LLMs (Large Language Models).  LLMs are a type of artificial intelligence that are trained on massive datasets of text and code.  They are capable of generating human-like text, translating languages, writing different kinds of creative content, and answering your questions in an informative way.\n",
       "\n",
       "The text snippets you provided seem to be from various EU regulations and legal documents focusing on topics like:\n",
       "\n",
       "* **Competency requirements for personnel performing assessments and verifications.** (Page 169)\n",
       "* **Data protection measures.** (Page 81)\n",
       "* **Risk assessment for medical devices.** (Page 178)\n",
       "* **EU regulations related to medical devices.** (Page 1)\n",
       "* **Communication requirements for companies.** (Page 15)\n",
       "\n",
       "These topics are not directly related to LLMs. To find information about LLMs, you might need to look for resources that specifically discuss artificial intelligence, machine learning, or natural language processing. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is a LLM?\"\n",
    "rag_chain_prompt_tuning = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | simplified_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "Markdown(rag_chain_prompt_tuning.invoke(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a Q&A chatbot that helps to answer the user's questions about a given document. Always follow these rules to answer the question:\n",
    "\n",
    "Use the following pieces of context to answer the questions.\n",
    "If the question is not related to the context, just say it is not related.\n",
    "If you don't know the answer to any of the questions, just say that you don't know, don't try to make up an answer.\n",
    "Always mention in which pages the information you give are found.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_template,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_runnable = RunnableLambda(lambda input: input[\"question\"])\n",
    "chat_history_runnable = RunnableLambda(lambda input: input[\"chat_history\"])\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": question_runnable | retriever | format_docs,\n",
    "        \"question\": question_runnable,\n",
    "        \"chat_history\": chat_history_runnable,\n",
    "    }\n",
    "    | question_answering_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuestionAnswerLoop():\n",
    "    print(\"Enter your question (type 'quit' to exit): \")\n",
    "    while True:\n",
    "        user_input = input(\"Enter your question (type 'quit' to exit): \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Exiting Q&A chat. Goodbye!\")\n",
    "            break\n",
    "        else:\n",
    "            chat_history.add_user_message(user_input)\n",
    "            response = rag_chain.invoke(\n",
    "                {\n",
    "                    \"question\": user_input, \n",
    "                    \"chat_history\": chat_history.messages\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Add the AI's response to the chat history\n",
    "            chat_history.add_ai_message(response)\n",
    "\n",
    "            # Print the response\n",
    "            print(\"Question: \" + user_input)\n",
    "            print(\"Answer: \" + response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question (type 'quit' to exit): \n",
      "Question: What is the medical devices regulation?\n",
      "Answer: The Medical Devices Regulation is a regulation that defines the requirements for medical devices and their use in the EU. It is intended to ensure that medical devices are safe and effective, and that they are used correctly. This information is found on page 5. \n",
      "\n",
      "Exiting Q&A chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "QuestionAnswerLoop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG limitations\n",
    "\n",
    "A basic RAG architecture will fetch documents from the database based on the similarity to the given user question. However, the user may not reference the document directly, e.g., \"What is in the first page?\" or \"Can you further explain the first question?\".\n",
    "\n",
    "To solve this, we can pass the question and the chat history to the LLM in an initial step to create the question that will be searched in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.schema import LLMResult\n",
    "\n",
    "class PrintLLMOutputCallbackHandler(BaseCallbackHandler):   # This is used so we can see what's going on behind the scenes\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs):\n",
    "        print(\"LLM Generated:\", response.generations[0][0].text)\n",
    "\n",
    "llm_with_logging = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", callbacks=[PrintLLMOutputCallbackHandler()]\n",
    ")\n",
    "\n",
    "metadata_field_info = [ # Setup the metadata that the LLM will be able to filter by\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        type=\"int\",\n",
    "        description=\"The page number of the document\",\n",
    "    )\n",
    "]\n",
    "\n",
    "document_content_description = \"Medical devices regulation\"\n",
    "\n",
    "query_with_history_template = \"\"\"\n",
    "You are an AI assistant that helps a user query a document.\n",
    "The user makes some questions and you create the queries to find the parts of document that are most relevant to the questions.\n",
    "The search will be performed by similarity so you need to provide a query similar to the contents that are in the document.\n",
    "You do not have the context of the document. You will be making the queries based on the questions and history to get the context from the document.\n",
    "This is the chat history of the conversation until now:\n",
    "<history>\n",
    "{chat_history}\n",
    "</history>\n",
    "\n",
    "Take into account the context of the chat history when preparing the query for the following question:\n",
    "Prepare a query for this question. Output only the query as a natural language question.\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "retriever_prompt_template = PromptTemplate.from_template(query_with_history_template)\n",
    "\n",
    "improved_retriever = SelfQueryRetriever.from_llm(\n",
    "    llm_with_logging,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Ask the LLM to generate a question based on the chat history and the user question\n",
    "# And ask the LLM to perform a search (can filter) based on the question generated\n",
    "improved_retriever_chain = retriever_prompt_template | llm_with_logging | StrOutputParser()\n",
    "\n",
    "# Normal QA RAG chain with the documents from the retrieval\n",
    "improved_rag_chain = question_answering_prompt | llm_with_logging | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_chat_history():   # Format the chat history to be used in the initial prompt (that will generate the database question)\n",
    "        result = \"\"\n",
    "        question_id = 1\n",
    "        for message in chat_history.messages:\n",
    "            if message.type == \"human\":\n",
    "                result += f\"{question_id}. Human: {message.content}\\n\"\n",
    "                question_id += 1\n",
    "            else:\n",
    "                result += f\"AI: {message.content}\\n\"\n",
    "        return result\n",
    "\n",
    "def invoke_improved_rag_chain(user_input):\n",
    "    llm_db_question = improved_retriever_chain.invoke({\"question\": user_input, \"chat_history\": formatted_chat_history()})\n",
    "    docs = improved_retriever.invoke(llm_db_question)\n",
    "    return improved_rag_chain.invoke(\n",
    "        {\n",
    "            \"context\": docs,\n",
    "            \"question\": llm_db_question,\n",
    "            \"chat_history\": chat_history.messages,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What is in the first page of the document?\",\n",
    "    \"Can you further explain the first question?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal RAG Chain\n",
      "\n",
      "\n",
      "Question: What is in the first page of the document?\n",
      "Answer: I'm sorry, but the context provided doesn't contain information about the first page of the document. \n",
      "\n",
      "Question: Can you further explain the first question?\n",
      "Answer: You asked: \"What is in the first page of the document?\"\n",
      "\n",
      "I understand you are asking for the content of the first page of the document I am using as context.  However, the context I have only provides information from specific pages (230, 231, 232, and 169).  There is no information about the first page. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_history = ChatMessageHistory() # reset the chat history\n",
    "\n",
    "print(\"Normal RAG Chain\\n\\n\")\n",
    "for user_input in queries:\n",
    "    chat_history.add_user_message(user_input)\n",
    "    response = rag_chain.invoke(\n",
    "        {\"question\": user_input, \"chat_history\": chat_history.messages}\n",
    "    )\n",
    "    chat_history.add_ai_message(response)\n",
    "    print(\"Question: \" + user_input)\n",
    "    print(\"Answer: \" + response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each query will generate 3 `LLM Generated` answers.\n",
    "- The first one is the actual question to pass to the database, which serves as a way to include the chat history in this question\n",
    "- The second is the LLM generated query to the database\n",
    "- The third is the actual answer from the LLM to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved RAG Chain\n",
      "\n",
      "\n",
      "---\n",
      "Question: What is in the first page of the document?\n",
      "LLM Generated: What is the content of the first page of the document? \n",
      "\n",
      "LLM Generated: ```json\n",
      "{\n",
      "    \"query\": \"Medical devices regulation\",\n",
      "    \"filter\": \"eq(\\\"page\\\", 1)\"\n",
      "}\n",
      "```\n",
      "LLM Generated: The first page of the document contains the title of the document \"REGULATION (EU) 2017/745 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\" and the subject matter and scope of the regulation. It also includes the definition of \"medical devices\" and \"accessories for medical devices\". This information is found on page 1 of the document. \n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "Question: Can you further explain the first question?\n",
      "LLM Generated: What is the content of the first page of the document \"REGULATION (EU) 2017/745 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\"? \n",
      "\n",
      "LLM Generated: ```json\n",
      "{\n",
      "    \"query\": \"REGULATION (EU) 2017/745 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\",\n",
      "    \"filter\": \"eq(\\\"page\\\", 1)\"\n",
      "}\n",
      "```\n",
      "LLM Generated: The first page of the document is a summary of the Regulation (EU) 2017/745. It outlines the main points of the regulation, including:\n",
      "\n",
      "* **Title:**  \"REGULATION (EU) 2017/745 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\" \n",
      "* **Purpose:**  The regulation sets rules about placing medical devices on the market within the European Union. \n",
      "* **Scope:**  It covers medical devices for human use, accessories for those devices, and clinical investigations related to them. \n",
      "* **Definition of \"medical devices\":**  It clarifies what constitutes a medical device.\n",
      "\n",
      "The first page acts as an introductory section to the regulation, providing a high-level overview of its contents. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_history = ChatMessageHistory()  # reset the chat history\n",
    "\n",
    "print(\"Improved RAG Chain\")\n",
    "for user_input in queries:\n",
    "    print(\"\\n\\n---\")\n",
    "    print(\"Question: \" + user_input)\n",
    "    chat_history.add_user_message(user_input)\n",
    "    response = invoke_improved_rag_chain(user_input)\n",
    "    chat_history.add_ai_message(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "This different architecture does not work 100% of the time, but it is a good improvement to the basic RAG architecture.\n",
    "\n",
    "In fact, this new architecture should be able to answer all of the questions that the basic RAG architecture can answer, and also some additional questions that the basic RAG architecture cannot answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
