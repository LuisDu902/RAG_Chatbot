{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Chatbots for Technical Documentation\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Load and split the document](#load-and-split-the-document)\n",
    "- [Generate and store the embeddings](#generate-and-store-the-embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This project involves implementing a retrieval augmented generation (RAG) with `LangChain` to create a chatbot for\n",
    "answering questions about technical documentation. The document chosen for this assignment was the following: The European Union Medical Device Regulation - Regulation (EU) 2017/745 (EU MDR). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Install the packages and dependencies to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install -qU langchain langchain-community langchain-chroma langchain-text-splitters unstructured sentence_transformers langchain-huggingface huggingface_hub pdfplumber langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='02017R0745 — EN — 09.07.2024 — 004.001 — 1\n",
      "This text is meant purely as a documentation tool and has no legal effect. The Union's institutions do not assume any liability\n",
      "for its contents. The authentic versions of the relevant acts, including their preambles, are those published in the Official\n",
      "Journal of the European Union and available in EUR-Lex. Those official texts are directly accessible through the links\n",
      "embedded in this document\n",
      "►B REGULATION (EU) 2017/745 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\n",
      "of 5 April 2017\n",
      "on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and\n",
      "Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC\n",
      "(Text with EEA relevance)\n",
      "(OJ L 117, 5.5.2017, p. 1)\n",
      "Amended by:\n",
      "Official Journal\n",
      "No page date\n",
      "►M1 Regulation (EU) 2020/561 of the European Parliament and of the L 130 18 24.4.2020\n",
      "Council of 23 April 2020\n",
      "►M2 Commission Delegated Regulation (EU) 2023/502 of 1 December 2022 L 70 1 8.3.2023' metadata={'source': 'document.pdf', 'file_path': 'document.pdf', 'page': 0, 'total_pages': 232, 'ModDate': \"D:20240808025522+02'00'\", 'Producer': '3-Heights(TM) PDF to PDF-A Converter Shell 4.7.24.2 (http://www.pdf-tools.com)', 'Title': 'CL2017R0745EN0040010.0001.3bi_cp 1..1', 'Author': 'Publications Office', 'Subject': ' ', 'Creator': 'Arbortext Advanced Print Publisher 10.0.1465/W Unicode', 'CreationDate': \"D:20240724041003-07'00'\", 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "# Using PDF document\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "loader = PDFPlumberLoader(\"document.pdf\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "pages = loader.load_and_split(text_splitter)\n",
    "\n",
    "print(pages[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and store the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and store the embeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=pages, embedding=embeddings, persist_directory=\"db\")\n",
    "# vectorstore = Chroma(persist_directory=\"db\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Retrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 16: which have been published in the Official Journal of the European\n",
      "Union, shall be presumed to be in conformity with the requirements\n",
      "of this Regulation covered by those standards or parts thereof.\n",
      "The first subparagraph shall also apply to system or process\n",
      "requirements to be fulfilled in accordance\n",
      "page 197: used, particularly as regards sterilisation and the relevant documents;\n",
      "and\n",
      "(e) the appropriate tests and trials which are to be carried out before,\n",
      "during and after manufacture, the frequency with which they are to\n",
      "take place, and the test equipment to be used; it shall be possible to\n",
      "trace back ad\n",
      "page 168: an initial start-up phase.\n",
      "1.6. Participation in coordination activities\n",
      "1.6.1. The notified body shall participate in, or ensure that its assessment\n",
      "personnel is informed of, any relevant standardisation activities and in\n",
      "the activities of the notified body coordination group referred to in\n",
      "Article\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"Describe the use of harmonised standards\")\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(\"page \" + str(doc.metadata[\"page\"] + 1) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ':\\n\\nThis is an extremely important question—how can a large proportion of people who may not speak the'},\n",
       " {'generated_text': ' to encourage good use of the same product\\n\\n(2)Where there is a strong preference among experts in'},\n",
       " {'generated_text': \", including the EU's 'Duke of Limbo' regulations as a 'guarantee that the countries\"},\n",
       " {'generated_text': ', one based on international practice of the EU.\\n\\n1. A national harmonised standard: whether under'},\n",
       " {'generated_text': '. I find that most of the information I have for a particular type of document has nothing to do with the'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2', max_length=1000, pad_token_id=50256, return_full_text=False)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "set_seed(42)\n",
    "generator(\"Describe the use of harmonised standards\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
      "\n",
      "filler context\n",
      "\n",
      "Question: filler question\n",
      "\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages\n",
    "\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This paragraph should be followed by a link to\n",
      "\n",
      "the appropriate guidance of this regulation by e-mail.\n",
      "\n",
      "1.6.3. Quality managers: Responsibilities\n",
      "\n",
      "1.6.4. It shall be the Director, by appointment\n",
      "\n",
      "of the Director of Quality Management and Policy Analysis and Co-ordinated with the\n",
      "\n",
      "Director of the Office of the Director of Quality Management and Policy Analysis to\n",
      "\n",
      "work\n",
      "\n",
      "through, monitor and assess the performance of quality managers\n",
      "\n",
      "and improve their role.\n",
      "\n",
      "2. Recommendations\n",
      "\n",
      "2.1. Quality managers: Responsits\n",
      "\n",
      "The Director of Quality Management and Policy Analysis shall:\n",
      "\n",
      "· review the quality management system and the assessment\n",
      "\n",
      "of the health of the quality management system,\n",
      "\n",
      "· evaluate the effectiveness and stability of its effectiveness\n",
      "\n",
      "· assess and manage the safety and effectiveness in the system\n",
      "\n",
      "· set out the objectives and standards laid down by\n",
      "\n",
      "the Director of Quality Management and Policy Analysis and the\n",
      "\n",
      "Coopering Guidelines\n",
      "\n",
      "· make recommendations on the provision of quality and related health measures\n",
      "\n",
      "· consider health monitoring of quality managers in the\n",
      "\n",
      "administration and the implementation of health and social services\n",
      "\n",
      "· provide advice, guidance and advice to decision makers and decision-makers\n",
      "\n",
      "Question: Where the evidence indicates that the quality management system\n",
      "\n",
      "is to be used; for the purpose for which the monitoring of these measures is in\n",
      "\n",
      "force;\n",
      "\n",
      "2.2. Acknowledgements and Exams\n",
      "\n",
      "Acknowledgements should be read in relation\n",
      "\n",
      "2.3. to Article 45 of Directive 2003/67/EC.\n",
      "\n",
      "The Director of the Office of the Director of Quality Management and Policy Analysis shall\n",
      "\n",
      "follow its responsibility to:\n",
      "\n",
      "Read and implement\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        page_number = doc.metadata[\"page\"] + 1 \n",
    "        content_with_page = f\"Page {page_number}:\\n{doc.page_content}\"\n",
    "        formatted_docs.append(content_with_page)\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(rag_chain.invoke(\"Describe the use of harmonised standards\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1565b76ee584ed9a48de5fe1efdb926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', continuous_update=False, description='Input:', layout=Layout(width='500px'), placeholder='Type …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38999851fb84e7f87d5f75304cc1b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f63f43afd84b9caff52093f8f3d043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Output(),), titles=('Complete generated prompt',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7e39013fa444179c6b1cfe68361fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Function that processes the user's question\n",
    "def process_input(user_input):\n",
    "    complete_prompt = f\"WIP\"\n",
    "    answer = rag_chain.invoke(user_input)\n",
    "    return answer, complete_prompt\n",
    "\n",
    "# Text input widget (for user question)\n",
    "text_input = widgets.Text(\n",
    "    description='Input:',\n",
    "    placeholder='Type something here...',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "# Primary output widget to display LLM's answer\n",
    "primary_output = widgets.Output()\n",
    "\n",
    "# Secondary output widget for the complete generated prompt (collapsible)\n",
    "secondary_output = widgets.Output()\n",
    "\n",
    "# Progress indicator (shown while processing)\n",
    "progress_indicator = widgets.Output()\n",
    "\n",
    "# Event handler for when Enter is pressed in the text input\n",
    "def on_text_submit(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        input_value = change.new.strip()\n",
    "        if input_value == \"\":\n",
    "            return\n",
    "\n",
    "        # Show the progress indicator\n",
    "        with progress_indicator:\n",
    "            progress_indicator.clear_output()\n",
    "            print(\"Processing... Please wait.\")\n",
    "\n",
    "        answer, complete_prompt = process_input(input_value)  # Processes the input\n",
    "\n",
    "        with primary_output:\n",
    "            primary_output.clear_output()       # Clears the previous output\n",
    "            print(f\"User Question: {input_value}\")\n",
    "            print(\"LLM Answer:\")\n",
    "            print(answer)\n",
    "\n",
    "        with secondary_output:\n",
    "            secondary_output.clear_output()     # Clears the previous output\n",
    "            print(\"Complete prompt:\")\n",
    "            print(complete_prompt)\n",
    "\n",
    "        # Hide the progress indicator after processing is complete\n",
    "        with progress_indicator:\n",
    "            progress_indicator.clear_output()\n",
    "\n",
    "        change.new = \"\"\n",
    "\n",
    "# Attach the event handler to the text input widget for Enter key submission\n",
    "text_input.continuous_update = False\n",
    "text_input.observe(on_text_submit, names='value', type=\"change\")\n",
    "\n",
    "# Make the complete generated prompt collapsible\n",
    "accordion = widgets.Accordion(children=[secondary_output])\n",
    "accordion.set_title(0, 'Complete generated prompt')\n",
    "\n",
    "# Display all the fields: text input, LLM's answer, complete prompt\n",
    "display(text_input, primary_output, accordion, progress_indicator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
