{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Chatbots for Technical Documentation\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Indexing](#indexing)\n",
    "- [Retriever](#retriever)\n",
    "- [Prompt](#prompt)\n",
    "- [LLM](#llm)\n",
    "- [RAG Chain](#rag-chain)\n",
    "- [Comparisons](#evaluation-metrics-and-comparison)\n",
    "- [Chat history](#chat-history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This project involves implementing a retrieval augmented generation (RAG) with *LangChain* to create a chatbot for\n",
    "answering questions about technical documentation. The document chosen for this assignment was the following: **The European Union Medical Device Regulation - Regulation (EU) 2017/745 (EU MDR)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Install the packages and dependencies to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install -qU langchain langchain-community langchain-chroma langchain-text-splitters unstructured sentence_transformers langchain-huggingface huggingface_hub pdfplumber langchain-google-genai ipywidgets python-dotenv lark chainlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Google's generative AI model is being used, ensure that the ``GOOGLE_API_KEY`` is securely stored in the ``.env`` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Firstly, we start by connecting to Google's generative AI embeddings model. The **Text Embeddings 004** model from Gemini is employed for the embedding generation, with the task_type set to *retrieval_document* to optimize embeddings for retrieval tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", task_type=\"retrieval_document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the indexing stage, we start by loading the PDF document and splitting it into manageable sections. To optimize execution time and improve efficiency, we store the vector store locally in a folder named \"db.\" This allows us to quickly access previously processed data without having to re-index the document each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "if os.path.exists(\"db\"):\n",
    "    vectorstore = Chroma(persist_directory=\"db\", embedding_function=embeddings)\n",
    "else:\n",
    "    loader = PDFPlumberLoader(\"document.pdf\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "    pages = loader.load_and_split(text_splitter)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=pages, embedding=embeddings, persist_directory=\"db\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever\n",
    "\n",
    "From the vector store, a retriever is created, configured to perform similarity searches and return the top 5 most relevant results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved document number : 0\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"Describe the use of harmonised standards\")\n",
    "\n",
    "print(\"Retrieved document number : \" + str(len(retrieved_docs)))\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(\"page \" + str(doc.metadata[\"page\"] + 1) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "We establish a structured format for the prompts sent to the LLM. This prompt format conveys the context while instructing the LLM to refrain from answering when it lacks confidence, thereby minimizing the risk of hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
      "\n",
      "Context: filler context\n",
      "\n",
      "Question: filler question\n",
      "\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages\n",
    "\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM utilized in this project is **Gemini 1.5 Flash**, recognized as Google Geminiâ€™s fastest multimodal model. It boasts an impressive context window of 1 million tokens, allowing for comprehensive understanding and processing of extensive inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['content', 'additional_kwargs', 'response_metadata', 'type', 'name', 'id', 'example', 'tool_calls', 'invalid_tool_calls', 'usage_metadata'])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## What is an LLM?\n",
       "\n",
       "LLM stands for **Large Language Model**. It's a type of artificial intelligence (AI) that excels at understanding and generating human-like text. \n",
       "\n",
       "Here's a breakdown:\n",
       "\n",
       "**What it is:**\n",
       "\n",
       "* **A complex algorithm:** LLMs are trained on massive datasets of text and code. They learn patterns and relationships within this data, enabling them to understand and generate text in a way that mimics human language.\n",
       "* **A powerful tool:** They can be used for various tasks, including:\n",
       "    * **Text generation:** Writing stories, articles, poems, and even code.\n",
       "    * **Translation:** Converting text from one language to another.\n",
       "    * **Summarization:** Condensing large amounts of text into concise summaries.\n",
       "    * **Question answering:** Providing answers to questions based on given information.\n",
       "    * **Dialogue generation:** Creating realistic and engaging chatbot conversations.\n",
       "\n",
       "**Key features:**\n",
       "\n",
       "* **Vast knowledge:** Trained on massive datasets, LLMs possess a broad understanding of various topics and domains.\n",
       "* **Contextual understanding:** They can analyze text in context, understanding the meaning of words and phrases based on their surroundings.\n",
       "* **Generative capabilities:** LLMs can generate new, creative text, often mimicking human writing styles.\n",
       "\n",
       "**Examples:**\n",
       "\n",
       "* **GPT-3 (Generative Pre-trained Transformer 3):** Developed by OpenAI, GPT-3 is one of the most well-known and powerful LLMs. It's used in various applications, including text generation, translation, and code completion.\n",
       "* **BERT (Bidirectional Encoder Representations from Transformers):** Developed by Google, BERT is another popular LLM used for tasks like question answering and sentiment analysis.\n",
       "* **LaMDA (Language Model for Dialogue Applications):** Developed by Google, LaMDA is specifically designed for conversational AI and powers Google's AI chatbot Bard.\n",
       "\n",
       "**Potential impact:**\n",
       "\n",
       "LLMs are revolutionizing the way we interact with technology and information. They have the potential to:\n",
       "\n",
       "* **Improve communication:** Facilitate more natural and efficient communication between humans and machines.\n",
       "* **Boost creativity:** Enable new forms of creative expression and content creation.\n",
       "* **Automate tasks:** Streamline tasks that require text processing and understanding, like customer service and content moderation.\n",
       "\n",
       "**However, LLMs also come with challenges:**\n",
       "\n",
       "* **Bias and misinformation:** Trained on vast amounts of data, LLMs can reflect existing biases and generate misinformation.\n",
       "* **Ethical concerns:** Questions arise about the responsible use of LLMs and their potential impact on society.\n",
       "\n",
       "**In conclusion:**\n",
       "\n",
       "LLMs are a powerful and rapidly evolving technology with the potential to revolutionize various aspects of our lives. As LLMs continue to develop, it's essential to understand their capabilities and limitations to ensure their responsible and ethical use.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "result = llm.invoke(\"What is an LLM?\")\n",
    "\n",
    "print(result.__dict__.keys())\n",
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, we can now define a RAG chain that takes a question, retrieves relevant documents, constructs a prompt, passes it into a model, and parses the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        page_number = doc.metadata[\"page\"] + 1 \n",
    "        content_with_page = f\"Page {page_number}:\\n{doc.page_content}\"\n",
    "        formatted_docs.append(content_with_page)\n",
    "    return \"\\n\\n\".join(formatted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Medical Devices Regulation (Regulation (EU) 2017/746) is a European Union regulation that establishes the rules for the safety and performance of medical devices. It sets out the requirements for manufacturers, importers, distributors, and other economic operators involved in the supply chain of medical devices. This regulation is found across the provided document, but specifically defined on pages 2 and 5. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the medical devices regulation?\"\n",
    "Markdown(rag_chain.invoke(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Temperature | Top P | Response |\n",
       "|-------------|-------|----------|\n",
       "| 0.1 | 0.1  | Harmonised standards are standards that have been published in the Official Journal of the European Union. These standards are presumed to be in conformity with the requirements of the Regulation. This information is found on Page 16. \n",
       "| 0.1 | 0.5  | Harmonised standards are standards that have been published in the Official Journal of the European Union and are presumed to be in conformity with the requirements of the Regulation. This information is found on Page 16. \n",
       "| 0.1 | 0.9  | Harmonised standards are standards that have been published in the Official Journal of the European Union. These standards are presumed to be in conformity with the requirements of the Regulation.  This information is found on page 16. \n",
       "| 0.5 | 0.1  | Harmonised standards are standards that have been published in the Official Journal of the European Union. These standards are presumed to be in conformity with the requirements of the Regulation.  This information is found on page 16. \n",
       "| 0.5 | 0.5  | Harmonised standards are standards that have been published in the Official Journal of the European Union and are presumed to be in conformity with the requirements of the Regulation. This information is found on page 16. \n",
       "| 0.5 | 0.9  | Harmonised standards are standards published in the Official Journal of the European Union that are presumed to be in conformity with the requirements of the Regulation. These standards can be applied to demonstrate conformity with general safety and performance requirements. This information is found on pages 16 and 147. \n",
       "| 1.0 | 0.1  | Harmonized standards are standards that have been published in the Official Journal of the European Union and are presumed to be in conformity with the requirements of the Regulation. This information is found on page 16. \n",
       "| 1.0 | 0.5  | Harmonised standards are standards that have been published in the Official Journal of the European Union. These standards are presumed to be in conformity with the requirements of the Regulation. This information is found on page 16. \n",
       "| 1.0 | 0.9  | Harmonised standards are standards that have been published in the Official Journal of the European Union. They are presumed to be in conformity with the requirements of the Regulation. This information can be found on page 16. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures = [0.1, 0.5, 1.0]\n",
    "top_ps = [0.1, 0.5, 0.9]\n",
    "\n",
    "results = \"| Temperature | Top P | Response |\\n\" + \"|-------------|-------|----------|\\n\"\n",
    "\n",
    "for temperature in temperatures:\n",
    "    for top_p in top_ps:\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=temperature, top_p=top_p)\n",
    "\n",
    "        query = \"What is harmonised standards?\"\n",
    "        response = rag_chain.invoke(query)\n",
    "\n",
    "        results += f\"| {temperature} | {top_p}  | {response}\"\n",
    "\n",
    "Markdown(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini vs GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2', max_length=1000, pad_token_id=50256, return_full_text=False)\n",
    "gpt2 = HuggingFacePipeline(pipeline=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "1. The regulations apply to general purposes to any medical device which is intended to be used\n",
       "\n",
       "for all purposes of the protection of human health, except insofar as such use\n",
       "\n",
       "may cause serious injury or disability in a person or in the process of a medical situation as\n",
       "\n",
       "part of an emergency.\n",
       "\n",
       "2. The regulations must also apply to, or in relation to, any specific use thereof that is to be limited\n",
       "\n",
       "to certain specific uses. In addition, they may specify the exact circumstances under which\n",
       "\n",
       "a specific use is likely to be prohibited, the procedures that are used as a control\n",
       "\n",
       "over incidental uses, and the general methods and activities employed.\n",
       "\n",
       "3. The regulation may also specify the specific conditions for\n",
       "\n",
       "presuming or reducing the harmful effects of substances or substances containing\n",
       "\n",
       "particles.\n",
       "\n",
       "4. The rules shall exclude the following situations:\n",
       "\n",
       "â€” where a specified percentage of this Article applies to other uses, if (1)\n",
       "\n",
       "the quantity of the used substances or the particular\n",
       "\n",
       "specific use is not a specific danger to the human health,\n",
       "\n",
       "(2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain_gpt2 = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | gpt2\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"What is the medical devices regulation?\"\n",
    "Markdown(rag_chain_gpt2.invoke(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "simplified_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The provided text snippets don't contain information about LLMs (Large Language Models). Therefore, it's impossible to answer the question from this provided text. \n",
       "\n",
       "LLMs are a type of artificial intelligence (AI) that are trained on massive amounts of text data. They are capable of generating human-like text, translating languages, writing different kinds of creative content, and answering your questions in an informative way. \n",
       "\n",
       "If you would like to learn more about LLMs, please let me know! I can provide you with information about their capabilities, applications, and limitations. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is a LLM?\"\n",
    "rag_chain_prompt_tuning = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | simplified_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "Markdown(rag_chain_prompt_tuning.invoke(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a Q&A chatbot that helps to answer the user's questions about a given document. Always follow these rules to answer the question:\n",
    "\n",
    "Use the following pieces of context to answer the questions.\n",
    "If the question is not related to the context, just say it is not related.\n",
    "If you don't know the answer to any of the questions, just say that you don't know, don't try to make up an answer.\n",
    "Always mention in which pages the information you give are found.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_template,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_runnable = RunnableLambda(lambda input: input[\"question\"])\n",
    "chat_history_runnable = RunnableLambda(lambda input: input[\"chat_history\"])\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": question_runnable | retriever | format_docs,\n",
    "        \"question\": question_runnable,\n",
    "        \"chat_history\": chat_history_runnable,\n",
    "    }\n",
    "    | question_answering_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuestionAnswerLoop():\n",
    "    print(\"Enter your question (type 'quit' to exit): \")\n",
    "    while True:\n",
    "        user_input = input(\"Enter your question (type 'quit' to exit): \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Exiting Q&A chat. Goodbye!\")\n",
    "            break\n",
    "        else:\n",
    "            chat_history.add_user_message(user_input)\n",
    "            response = rag_chain.invoke(\n",
    "                {\n",
    "                    \"question\": user_input, \n",
    "                    \"chat_history\": chat_history.messages\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Add the AI's response to the chat history\n",
    "            chat_history.add_ai_message(response)\n",
    "\n",
    "            # Print the response\n",
    "            print(\"Question: \" + user_input)\n",
    "            print(\"Answer: \" + response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question (type 'quit' to exit): \n",
      "Question: What is the medical devices regulation?\n",
      "Answer: The Medical Devices Regulation (MDR) is a comprehensive set of rules governing the safety and performance of medical devices in the European Union.  It's a complex piece of legislation, but here's a breakdown of key points based on your provided text snippets:\n",
      "\n",
      "**Purpose:**\n",
      "\n",
      "* The MDR aims to ensure the safety and effectiveness of medical devices used on humans, while promoting innovation and patient safety.\n",
      "* It covers a wide range of medical devices, from simple instruments to complex implants and software.\n",
      "\n",
      "**Key Elements:**\n",
      "\n",
      "* **Definition of \"medical device\":**  The text defines a medical device as any instrument, apparatus, appliance, software, implant, etc. intended by the manufacturer to be used for a specific medical purpose. Examples include:\n",
      "    * **Diagnosis:** Identifying a disease\n",
      "    * **Prevention:**  Stopping a disease from developing\n",
      "    * **Monitoring:** Tracking a patient's condition\n",
      "    * **Treatment:**  Addressing a disease or injury\n",
      "    * **Alleviation:** Reducing symptoms\n",
      "* **Scope of Application:** The MDR applies to all medical devices placed on the EU market, including those manufactured outside the EU.\n",
      "* **Harmonization:**  The MDR seeks to harmonize medical device regulations across EU member states, making it easier for manufacturers to market their products throughout the Union.\n",
      "* **Risk Management:** The MDR emphasizes a strong focus on risk management, requiring manufacturers to identify and assess potential hazards associated with their devices. \n",
      "* **Clinical Evidence:**  Manufacturers must provide clinical evidence demonstrating the safety and performance of their devices before they can be placed on the market.\n",
      "* **Post-Market Surveillance:** The MDR includes provisions for post-market surveillance to monitor the safety and performance of devices once they are in use.\n",
      "* **Transparency:** The MDR promotes transparency by requiring manufacturers to provide information about their devices to regulators and the public.\n",
      "* **Cooperation:** The MDR encourages cooperation between manufacturers, regulators, and healthcare professionals to ensure the safety and effectiveness of medical devices.\n",
      "\n",
      "**Additional Information:**\n",
      "\n",
      "* **Interaction with other regulations:** The MDR interacts with other EU regulations, such as the Machinery Directive (mentioned in the text) and the In Vitro Diagnostic Medical Devices Regulation (IVDR).\n",
      "* **Impact on manufacturers:** The MDR has significant implications for manufacturers of medical devices, requiring them to comply with a range of new requirements. \n",
      "\n",
      "**Overall, the MDR is a significant piece of legislation that aims to improve the safety and effectiveness of medical devices in the European Union. It requires manufacturers to take a comprehensive approach to risk management, provide clinical evidence, and engage in post-market surveillance.** \n",
      "\n",
      "Question: \n",
      "Answer: The question is missing. Please provide the question you want answered based on the provided text. \n",
      "\n",
      "For example, you could ask:\n",
      "\n",
      "* **What articles in Council Directive 90/385/EEC are mentioned on Page 232?**\n",
      "* **What articles in this Regulation correspond to Article 10 in Council Directive 90/385/EEC?**\n",
      "* **Which articles from Council Directive 93/42/EEC are not included in the table?** \n",
      "\n",
      "Once you provide a question, I can give you a helpful answer based on the information in the document. \n",
      "\n",
      "Exiting Q&A chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "QuestionAnswerLoop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG limitations\n",
    "\n",
    "A basic RAG architecture will fetch documents from the database based on the similarity to the given user question. However, the user may not reference the document directly, e.g., \"What is in the first page?\" or \"Can you further explain the first question?\".\n",
    "\n",
    "To solve this, we can pass the question and the chat history to the LLM in an initial step to create the question that will be searched in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.schema import LLMResult\n",
    "\n",
    "class PrintLLMOutputCallbackHandler(BaseCallbackHandler):   # This is used so we can see what's going on behind the scenes\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs):\n",
    "        print(\"LLM Generated:\", response.generations[0][0].text)\n",
    "\n",
    "llm_with_logging = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", callbacks=[PrintLLMOutputCallbackHandler()]\n",
    ")\n",
    "\n",
    "metadata_field_info = [ # Setup the metadata that the LLM will be able to filter by\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        type=\"int\",\n",
    "        description=\"The page number of the document\",\n",
    "    )\n",
    "]\n",
    "\n",
    "document_content_description = \"Medical devices regulation\"\n",
    "\n",
    "query_with_history_template = \"\"\"\n",
    "You are an AI assistant that helps a user query a document.\n",
    "The user makes some questions and you create the queries to find the parts of document that are most relevant to the questions.\n",
    "The search will be performed by similarity so you need to provide a query similar to the contents that are in the document.\n",
    "You do not have the context of the document. You will be making the queries based on the questions and history to get the context from the document.\n",
    "This is the chat history of the conversation until now:\n",
    "<history>\n",
    "{chat_history}\n",
    "</history>\n",
    "\n",
    "Take into account the context of the chat history when preparing the query for the following question:\n",
    "Prepare a query for this question. Output only the query as a natural language question.\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "retriever_prompt_template = PromptTemplate.from_template(query_with_history_template)\n",
    "\n",
    "improved_retriever = SelfQueryRetriever.from_llm(\n",
    "    llm_with_logging,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Ask the LLM to generate a question based on the chat history and the user question\n",
    "# And ask the LLM to perform a search (can filter) based on the question generated\n",
    "improved_retriever_chain = retriever_prompt_template | llm_with_logging | StrOutputParser()\n",
    "\n",
    "# Normal QA RAG chain with the documents from the retrieval\n",
    "improved_rag_chain = question_answering_prompt | llm_with_logging | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_chat_history():   # Format the chat history to be used in the initial prompt (that will generate the database question)\n",
    "        result = \"\"\n",
    "        question_id = 1\n",
    "        for message in chat_history.messages:\n",
    "            if message.type == \"human\":\n",
    "                result += f\"{question_id}. Human: {message.content}\\n\"\n",
    "                question_id += 1\n",
    "            else:\n",
    "                result += f\"AI: {message.content}\\n\"\n",
    "        return result\n",
    "\n",
    "def invoke_improved_rag_chain(user_input):\n",
    "    llm_db_question = improved_retriever_chain.invoke({\"question\": user_input, \"chat_history\": formatted_chat_history()})\n",
    "    docs = improved_retriever.invoke(llm_db_question)\n",
    "    return improved_rag_chain.invoke(\n",
    "        {\n",
    "            \"context\": docs,\n",
    "            \"question\": llm_db_question,\n",
    "            \"chat_history\": chat_history.messages,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What is in the first page of the document?\",\n",
    "    \"Can you further explain the first question?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal RAG Chain\n",
      "\n",
      "\n",
      "Question: What is in the first page of the document?\n",
      "Answer: Unfortunately, without the context of the entire document, it's impossible to know what's on the first page. The provided snippets seem to be from different sections within a larger document, likely a legal or technical regulation. \n",
      "\n",
      "To determine the content of the first page, you'll need the full document. \n",
      "\n",
      "Question: Can you further explain the first question?\n",
      "Answer: It seems you are asking about the \"Correlation Table\" in Annex XVII of Regulation 02017R0745. This table shows how provisions from two older directives (Council Directive 90/385/EEC and Council Directive 93/42/EEC) are incorporated into this newer regulation. \n",
      "\n",
      "**Understanding the Table:**\n",
      "\n",
      "* **Columns:** The table has three columns:\n",
      "    * **Council Directive 90/385/EEC:**  Lists articles from the first directive.\n",
      "    * **Council Directive 93/42/EEC:** Lists articles from the second directive.\n",
      "    * **This Regulation:** Lists articles from the newer regulation (02017R0745).\n",
      "\n",
      "* **Correlation:** Each row shows how an article from one of the directives relates to an article in the newer regulation.  A dash \"-\" indicates that the article is not included in the new regulation.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Looking at the first row, you can see:\n",
      "\n",
      "* **Article 1(1) of Directive 90/385/EEC** corresponds to **Article 1(1) of Directive 93/42/EEC** and **Article 1(1) of Regulation 02017R0745.**\n",
      "\n",
      "**What is the First Question?**\n",
      "\n",
      "To answer your question more specifically, please provide the actual first question from the text you are referring to. I need to know what the first question is to help you understand it better. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_history = ChatMessageHistory() # reset the chat history\n",
    "\n",
    "print(\"Normal RAG Chain\\n\\n\")\n",
    "for user_input in queries:\n",
    "    chat_history.add_user_message(user_input)\n",
    "    response = rag_chain.invoke(\n",
    "        {\"question\": user_input, \"chat_history\": chat_history.messages}\n",
    "    )\n",
    "    chat_history.add_ai_message(response)\n",
    "    print(\"Question: \" + user_input)\n",
    "    print(\"Answer: \" + response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each query will generate 3 `LLM Generated` answers.\n",
    "- The first one is the actual question to pass to the database, which serves as a way to include the chat history in this question\n",
    "- The second is the LLM generated query to the database\n",
    "- The third is the actual answer from the LLM to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved RAG Chain\n",
      "\n",
      "\n",
      "---\n",
      "Question: What is in the first page of the document?\n",
      "LLM Generated: What is on the first page of this document? \n",
      "\n",
      "LLM Generated: ```json\n",
      "{\n",
      "    \"query\": \"\",\n",
      "    \"filter\": \"eq(\\\"page\\\", 1)\"\n",
      "}\n",
      "```\n",
      "LLM Generated: The first page of the document contains the title of the document, which is \"REGULATION (EU) 2017/745 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 5 April 2017 on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (Text with EEA relevance)\". It also includes information about the subject matter and scope of the regulation. \n",
      "This information can be found on page 1 of the document. \n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "Question: Can you further explain the first question?\n",
      "LLM Generated: What is the subject matter and scope of the regulation \"REGULATION (EU) 2017/745 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 5 April 2017 on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (Text with EEA relevance)\"? \n",
      "\n",
      "LLM Generated: ```json\n",
      "{\n",
      "    \"query\": \"REGULATION (EU) 2017/745 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 5 April 2017 on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (Text with EEA relevance)\",\n",
      "    \"filter\": \"NO_FILTER\"\n",
      "}\n",
      "```\n",
      "LLM Generated: The first page of the document is a legal document called a regulation. It outlines the rules for placing medical devices on the market within the European Union. The document is titled \"REGULATION (EU) 2017/745\" and was enacted on April 5th, 2017. It also states that it amends previous directives and regulations regarding medical devices.  \n",
      "\n",
      "The first page also includes a disclaimer stating that the text is for informational purposes only and has no legal effect. The document directs readers to the Official Journal of the European Union for the authoritative versions of the acts. \n",
      "\n",
      "You can find this information on page 1 of the document. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_history = ChatMessageHistory()  # reset the chat history\n",
    "\n",
    "print(\"Improved RAG Chain\")\n",
    "for user_input in queries:\n",
    "    print(\"\\n\\n---\")\n",
    "    print(\"Question: \" + user_input)\n",
    "    chat_history.add_user_message(user_input)\n",
    "    response = invoke_improved_rag_chain(user_input)\n",
    "    chat_history.add_ai_message(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "This different architecture does not work 100% of the time, but it is a good improvement to the basic RAG architecture.\n",
    "\n",
    "In fact, this new architecture should be able to answer all of the questions that the basic RAG architecture can answer, and also some additional questions that the basic RAG architecture cannot answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
