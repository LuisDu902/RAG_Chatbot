{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Chatbots for Technical Documentation\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Indexing](#load-and-split-the-document)\n",
    "- []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This project involves implementing a retrieval augmented generation (RAG) with *LangChain* to create a chatbot for\n",
    "answering questions about technical documentation. The document chosen for this assignment was the following: **The European Union Medical Device Regulation - Regulation (EU) 2017/745 (EU MDR)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Install the packages and dependencies to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "%pip install -qU langchain langchain-community langchain-chroma langchain-text-splitters unstructured sentence_transformers langchain-huggingface huggingface_hub pdfplumber langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Google's generative AI model is being used, ensure that the ``GOOGLE_API_KEY`` is securely stored in the ``.env`` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Firstly, we start by connecting to Google's generative AI embeddings model. The **Text Embeddings 004** model from Gemini is employed for the embedding generation, with the task_type set to *retrieval_document* to optimize embeddings for retrieval tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, \n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", task_type=\"retrieval_document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the indexing stage, we start by loading the PDF document and splitting it into manageable sections. To optimize execution time and improve efficiency, we store the vector store locally in a folder named \"db.\" This allows us to quickly access previously processed data without having to re-index the document each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "if os.path.exists(\"db\"): \n",
    "    vectorstore = Chroma(persist_directory=\"db\", embedding_function=embeddings)\n",
    "else:\n",
    "    loader = PDFPlumberLoader(\"document.pdf\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "    pages = loader.load_and_split(text_splitter)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=pages, embedding=embeddings, persist_directory=\"db\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Retriever\n",
    "\n",
    "From the vector store, a retriever is created, configured to perform similarity searches and return the top 5 most relevant results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved document number : 5\n",
      "page 169: of those staff, in order to ensure that personnel who carry out and perform\n",
      "assessment and verification operations are competent to fulfil the tasks\n",
      "required of them.\n",
      "page 127: ated with each hazard as well as the overall residual risk is judged\n",
      "acceptable. In selecting the most appropriate solutions, manufacturers\n",
      "shall, in the following order of priority:\n",
      "(a) eliminate or reduce risks as far as possible through safe design and\n",
      "manufacture;\n",
      "page 196: conformity assessment procedures,\n",
      "— identification of applicable general safety and performance\n",
      "requirements and solutions to fulfil those requirements, taking\n",
      "applicable CS and, where opted for, harmonised standards or\n",
      "other adequate solutions into account,\n",
      "— risk management as referred to in Secti\n",
      "page 147: purpose, and shall include a justification, validation and verification of the\n",
      "solutions adopted to meet those requirements. The demonstration of\n",
      "conformity shall include:\n",
      "(a) the general safety and performance requirements that apply to the device\n",
      "and an explanation as to why others do not apply;\n",
      "(\n",
      "page 179: — carry out the appropriate examinations and tests in order to verify that\n",
      "the solutions adopted by the manufacturer meet the general safety and\n",
      "performance requirements set out in Annex I. Such examinations and\n",
      "tests shall include all tests necessary to verify that the manufacturer\n",
      "has in fact appl\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"Describe the use of harmonised standards\")\n",
    "\n",
    "print(\"Retrieved document number : \" + str(len(retrieved_docs)))\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(\"page \" + str(doc.metadata[\"page\"] + 1) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "We establish a structured format for the prompts sent to the LLM. This prompt format conveys the context while instructing the LLM to refrain from answering when it lacks confidence, thereby minimizing the risk of hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
      "\n",
      "filler context\n",
      "\n",
      "Question: filler question\n",
      "\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages\n",
    "\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM utilized in this project is **Gemini 1.5 Flash**, recognized as Google Gemini’s fastest multimodal model. It boasts an impressive context window of 1 million tokens, allowing for comprehensive understanding and processing of extensive inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['content', 'additional_kwargs', 'response_metadata', 'type', 'name', 'id', 'example', 'tool_calls', 'invalid_tool_calls', 'usage_metadata'])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "LLM stands for **Large Language Model**. \n",
       "\n",
       "Here's a breakdown:\n",
       "\n",
       "**What is a Large Language Model?**\n",
       "\n",
       "* **A type of artificial intelligence (AI):** LLMs are a specific type of AI that excels at understanding and generating human-like text.\n",
       "* **Trained on massive datasets:**  They are trained on enormous amounts of text data, like books, articles, code, and websites. This training allows them to learn patterns and relationships in language.\n",
       "* **Capable of many language tasks:** LLMs can perform a wide range of tasks, including:\n",
       "    * **Text generation:** Writing stories, poems, articles, and even code.\n",
       "    * **Translation:** Converting text from one language to another.\n",
       "    * **Summarization:** Condensing large amounts of text into concise summaries.\n",
       "    * **Question answering:** Providing answers to complex questions.\n",
       "    * **Code generation:** Writing code in various programming languages.\n",
       "    * **Conversation:** Engaging in natural-sounding conversations.\n",
       "\n",
       "**Examples of LLMs:**\n",
       "\n",
       "* **GPT-3 (Generative Pre-trained Transformer 3):** Developed by OpenAI, known for its impressive text generation capabilities.\n",
       "* **LaMDA (Language Model for Dialogue Applications):** Developed by Google, specifically designed for conversational AI.\n",
       "* **BERT (Bidirectional Encoder Representations from Transformers):** Developed by Google, excels at understanding the meaning of text.\n",
       "\n",
       "**Key Features of LLMs:**\n",
       "\n",
       "* **Deep learning:** They use complex neural networks to process information.\n",
       "* **Contextual understanding:** They can understand the meaning of words based on their context in a sentence or paragraph.\n",
       "* **Generative capabilities:** They can create new text that is coherent and grammatically correct.\n",
       "* **Scalability:** They can be trained on massive datasets and deployed on powerful hardware.\n",
       "\n",
       "**Implications of LLMs:**\n",
       "\n",
       "LLMs are revolutionizing various fields, including:\n",
       "\n",
       "* **Content creation:** Automating writing tasks, improving content quality, and making content more accessible.\n",
       "* **Customer service:** Providing automated support, answering questions, and resolving issues.\n",
       "* **Education:** Personalizing learning experiences and providing real-time feedback.\n",
       "* **Research:** Analyzing large amounts of text data and generating insights.\n",
       "\n",
       "**Important Considerations:**\n",
       "\n",
       "* **Bias:** LLMs can reflect biases present in the training data, so it's crucial to address potential biases.\n",
       "* **Ethical implications:** The potential for misuse, such as generating fake news or impersonating individuals, needs careful consideration.\n",
       "\n",
       "Overall, LLMs are powerful tools with the potential to change the way we interact with language and information. Their capabilities and implications are constantly evolving, making them a fascinating and rapidly developing area of AI research. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "result = llm.invoke(\"What is an LLM?\")\n",
    "\n",
    "print(result.__dict__.keys())\n",
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, we can now define a RAG chain that takes a question, retrieves relevant documents, constructs a prompt, passes it into a model, and parses the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        page_number = doc.metadata[\"page\"] + 1 \n",
    "        content_with_page = f\"Page {page_number}:\\n{doc.page_content}\"\n",
    "        formatted_docs.append(content_with_page)\n",
    "    return \"\\n\\n\".join(formatted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Medical Devices Regulation (MDR) is a regulation of the European Union that governs the safety and performance of medical devices. It was adopted in 2017 and came into full effect on May 26, 2021. The MDR is a comprehensive piece of legislation that sets out requirements for the design, manufacture, and marketing of medical devices, including the use of a unique device identification (UDI) system. The MDR can be found in its entirety in Regulation (EU) 2017/745, which is referenced throughout the provided text. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the medical devices regulation?\"\n",
    "Markdown(rag_chain.invoke(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Temperature | Top P | Response |\n",
       "|-------------|-------|----------|\n",
       "| 0.1 | 0.1  | Harmonised standards are standards published in the Official Journal of the European Union. These standards are presumed to be in conformity with the requirements of the Regulation. This information is found on page 16. \n",
       "| 0.1 | 0.5  | Harmonized standards are standards published in the Official Journal of the European Union. These standards are presumed to be in conformity with the requirements of the Regulation. This information is found on page 16. \n",
       "| 0.1 | 1.0  | Harmonised standards are standards published in the Official Journal of the European Union. These standards are presumed to be in conformity with the requirements of the Regulation. The answer can be found on page 16. \n",
       "| 0.5 | 0.1  | Harmonised standards are standards published in the Official Journal of the European Union. These standards are presumed to be in conformity with the requirements of the regulation. This information is found on page 16. \n",
       "| 0.5 | 0.5  | Harmonized standards are standards published in the Official Journal of the European Union. They are presumed to be in conformity with the requirements of the Regulation. This information is found on page 16. \n",
       "| 0.5 | 1.0  | Harmonized standards are standards published in the Official Journal of the European Union. These standards are presumed to be in conformity with the requirements of the Regulation. This information is found on page 16. \n",
       "| 1.0 | 0.1  | Harmonised standards are standards that have been published in the Official Journal of the European Union. These standards are presumed to be in conformity with the requirements of the Regulation. This information is found on page 16. \n",
       "| 1.0 | 0.5  | Harmonised standards are standards that have been published in the Official Journal of the European Union. These standards are presumed to be in conformity with the requirements of the Regulation. This information is found on page 16. \n",
       "| 1.0 | 1.0  | Harmonised standards are standards that have been published in the Official Journal of the European Union. These standards are presumed to be in conformity with the requirements of the Regulation. This information is found on page 16 of the document. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures = [0.1, 0.5, 1.0]\n",
    "top_ps = [0.1, 0.5, 0.9]\n",
    "\n",
    "results = \"| Temperature | Top P | Response |\\n\" + \"|-------------|-------|----------|\\n\"\n",
    "\n",
    "for temperature in temperatures:\n",
    "    for top_p in top_ps:\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=temperature, top_p=top_p)\n",
    "\n",
    "        query = \"What is harmonised standards?\"\n",
    "        response = rag_chain.invoke(query)\n",
    "\n",
    "        results += f\"| {temperature} | {top_p}  | {response}\"\n",
    "\n",
    "Markdown(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini vs GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e49ce0c958435099fb8fcc5ffe2a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  78%|#######8  | 430M/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\luisd\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a892c4f0a64f4085f692b8837ac736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfea998ade34bcabef297102af16734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b39f0750dc46df8e5ba4edd9c4059a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9974a58fedba4f55a9e80b149ab2acfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed96a0be2204422780cb43ba53fe97c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2', max_length=1000, pad_token_id=50256, return_full_text=False)\n",
    "gpt2 = HuggingFacePipeline(pipeline=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "An individual or institution is not a person or entity and is not subject to either the Medical\n",
       "\n",
       "Device Regulation or the medical device regulations. It is essential to\n",
       "\n",
       "list all the necessary devices and procedures in the medical device\n",
       "\n",
       "registry, to ensure that the necessary devices and procedures are\n",
       "\n",
       "considered in relation to the specific individual who qualifies for the licensing\n",
       "\n",
       "of such medical devices under the Regulation and to maintain their functionality and\n",
       "\n",
       "accuracy. When the Medical Device Regulation applies to an individual, his\n",
       "\n",
       "health information must be clearly identified at the time he receives his\n",
       "\n",
       "license or a copy of the license must be provided to the patient.\n",
       "\n",
       "Question: What is the'medical devices policy', \"medical device\n",
       "\n",
       "licensing policy\", which applies to consumers seeking\n",
       "\n",
       "medical devices license?\n",
       "\n",
       "Find the right answer of yes or no for the Medical Device, Medical\n",
       "\n",
       "Device Regulation, and Medical Device licensing policy under your choice of\n",
       "\n",
       "the'medical device policy' under your choice of\n",
       "\n",
       "the'medical device licensing policy' at the end of your order of\n",
       "\n",
       "licensing.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain_gpt2 = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | gpt2\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"What is the medical devices regulation?\"\n",
    "Markdown(rag_chain_gpt2.invoke(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The provided text snippets don't contain any direct mention or definition of what an LLM is.  These snippets are excerpts from various European Union Regulations and Directives. \n",
       "\n",
       "**LLM stands for Large Language Model.**  It is a type of artificial intelligence that is trained on massive amounts of text data and can generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
       "\n",
       "The text you provided likely discusses various regulations regarding data protection, technical and organizational measures, and compliance with EU directives. While LLMs are capable of analyzing text and generating outputs, the provided text doesn't provide any information on this specific technology. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is a LLM?\"\n",
    "rag_chain_prompt_tuning = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "Markdown(rag_chain_prompt_tuning.invoke(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a Q&A chatbot that helps to answer the user's questions about a given document. Always follow these rules to answer the question:\n",
    "\n",
    "Use the following pieces of context to answer the questions.\n",
    "If the question is not related to the context, just say it is not related.\n",
    "If you don't know the answer to any of the questions, just say that you don't know, don't try to make up an answer.\n",
    "Always mention in which pages the information you give are found.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_template,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_runnable = RunnableLambda(lambda input: input[\"question\"])\n",
    "chat_history_runnable = RunnableLambda(lambda input: input[\"chat_history\"])\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "            {\n",
    "                \"context\": question_runnable | retriever | format_docs,\n",
    "                \"question\": question_runnable,\n",
    "                \"chat_history\": chat_history_runnable,\n",
    "            }\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuestionAnswerLoop():\n",
    "    print(\"Enter your question (type 'quit' to exit): \")\n",
    "    while True:\n",
    "        user_input = input(\"Enter your question (type 'quit' to exit): \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Exiting Q&A chat. Goodbye!\")\n",
    "            break\n",
    "        else:\n",
    "            response = rag_chain.invoke(\n",
    "                {\n",
    "                    \"question\": user_input, \n",
    "                    \"chat_history\": chat_history.messages\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Add the AI's response to the chat history\n",
    "            chat_history.add_ai_message(response)\n",
    "\n",
    "            # Print the response\n",
    "            print(\"Question: \" + user_input)\n",
    "            print(\"Answer: \" + response)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question (type 'quit' to exit): \n",
      "Question: what is MDR\n",
      "Answer: MDR stands for **Medical Devices Regulation**. \n",
      "\n",
      "Specifically, it refers to **Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices**, which replaced the older Directives for medical devices (90/385/EEC and 93/42/EEC) in Europe. This Regulation sets a new framework for the safety and performance standards of medical devices in the European Union.\n",
      "\n",
      "You can see this abbreviation mentioned on the pages you provided. For example, on Page 3, you can read \"The requirements of **Regulation (EU) 2017/746** shall apply to the in vitro diagnostic medical device part of the device.\"\n",
      "\n",
      "The text you provided appears to be excerpts from documents related to MDR. \n",
      "\n",
      "Question: what is MDR\n",
      "Answer: MDR stands for **Medical Device Regulation**. It refers to **Regulation (EU) 2017/745** of the European Parliament and of the Council of 5 April 2017 on medical devices. \n",
      "\n",
      "It's the current legal framework for medical devices within the European Union, replacing the previous Directives 90/385/EEC and 93/42/EEC. \n",
      "\n",
      "Exiting Q&A chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "QuestionAnswerLoop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
