{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Chatbots for Technical Documentation\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Indexing](#indexing)\n",
    "- [Retriever](#retriever)\n",
    "- [Prompt](#prompt)\n",
    "- [LLM](#llm)\n",
    "- [RAG Chain](#rag-chain)\n",
    "- [Comparisons](#evaluation-metrics-and-comparison)\n",
    "- [Chat history](#chat-history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This project involves implementing a retrieval augmented generation (RAG) with *LangChain* to create a chatbot for\n",
    "answering questions about technical documentation. The document chosen for this assignment was the following: **The European Union Medical Device Regulation - Regulation (EU) 2017/745 (EU MDR)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Install the packages and dependencies to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install -qU langchain langchain-community langchain-chroma langchain-text-splitters unstructured sentence_transformers langchain-huggingface huggingface_hub pdfplumber langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Google's generative AI model is being used, ensure that the ``GOOGLE_API_KEY`` is securely stored in the ``.env`` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Firstly, we start by connecting to Google's generative AI embeddings model. The **Text Embeddings 004** model from Gemini is employed for the embedding generation, with the task_type set to *retrieval_document* to optimize embeddings for retrieval tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", task_type=\"retrieval_document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the indexing stage, we start by loading the PDF document and splitting it into manageable sections. To optimize execution time and improve efficiency, we store the vector store locally in a folder named \"db.\" This allows us to quickly access previously processed data without having to re-index the document each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "if os.path.exists(\"db\"): \n",
    "    vectorstore = Chroma(persist_directory=\"db\", embedding_function=embeddings)\n",
    "else:\n",
    "    loader = PDFPlumberLoader(\"document.pdf\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "    pages = loader.load_and_split(text_splitter)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=pages, embedding=embeddings, persist_directory=\"db\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever\n",
    "\n",
    "From the vector store, a retriever is created, configured to perform similarity searches and return the top 5 most relevant results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved document number : 5\n",
      "page 169: of those staff, in order to ensure that personnel who carry out and perform\n",
      "assessment and verification operations are competent to fulfil the tasks\n",
      "required of them.\n",
      "page 196: conformity assessment procedures,\n",
      "— identification of applicable general safety and performance\n",
      "requirements and solutions to fulfil those requirements, taking\n",
      "applicable CS and, where opted for, harmonised standards or\n",
      "other adequate solutions into account,\n",
      "— risk management as referred to in Secti\n",
      "page 147: purpose, and shall include a justification, validation and verification of the\n",
      "solutions adopted to meet those requirements. The demonstration of\n",
      "conformity shall include:\n",
      "(a) the general safety and performance requirements that apply to the device\n",
      "and an explanation as to why others do not apply;\n",
      "(\n",
      "page 153: description of the conformity assessment procedure performed and identifi­\n",
      "cation of the certificate or certificates issued;\n",
      "9. Where applicable, additional information;\n",
      "10. Place and date of issue of the declaration, name and function of the person\n",
      "who signed it as well as an indication for, and on\n",
      "page 176: including personnel are utilised for each of the tasks of the assessment.\n",
      "The allocation of tasks required to be carried out as part of the conformity\n",
      "assessment and any changes subsequently made to this allocation shall be\n",
      "documented.\n",
      "4.5. Conformity assessment activities\n",
      "4.5.1. General\n",
      "The notifie\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"Describe the use of harmonised standards\")\n",
    "\n",
    "print(\"Retrieved document number : \" + str(len(retrieved_docs)))\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(\"page \" + str(doc.metadata[\"page\"] + 1) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "We establish a structured format for the prompts sent to the LLM. This prompt format conveys the context while instructing the LLM to refrain from answering when it lacks confidence, thereby minimizing the risk of hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Use three sentences maximum and keep the answer as concise as possible. Mention in which pages the answer is found.\n",
      "\n",
      "Context: filler context\n",
      "\n",
      "Question: filler question\n",
      "\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages\n",
    "\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM utilized in this project is **Gemini 1.5 Flash**, recognized as Google Gemini’s fastest multimodal model. It boasts an impressive context window of 1 million tokens, allowing for comprehensive understanding and processing of extensive inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['content', 'additional_kwargs', 'response_metadata', 'type', 'name', 'id', 'example', 'tool_calls', 'invalid_tool_calls', 'usage_metadata'])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "LLM stands for **Large Language Model**. It's a type of artificial intelligence (AI) that excels at understanding and generating human-like text. \n",
       "\n",
       "Here's a breakdown:\n",
       "\n",
       "**What it is:**\n",
       "\n",
       "* **A complex statistical model:** LLMs are trained on massive datasets of text and code, learning patterns and relationships within the data. \n",
       "* **Predictive in nature:** They predict the next word in a sequence, based on the context of the preceding words. This allows them to generate coherent and contextually relevant text.\n",
       "* **Capable of various tasks:** LLMs can perform a wide range of natural language processing (NLP) tasks, including:\n",
       "    * **Text generation:** Writing stories, poems, articles, code, etc.\n",
       "    * **Translation:** Translating text between languages.\n",
       "    * **Summarization:** Condensing large amounts of text into concise summaries.\n",
       "    * **Question answering:** Providing answers to questions based on given text.\n",
       "    * **Dialogue generation:** Engaging in conversations with humans.\n",
       "    * **Code generation:** Writing computer code.\n",
       "\n",
       "**Examples:**\n",
       "\n",
       "* **GPT-3 (Generative Pre-trained Transformer 3):** Developed by OpenAI, it's known for its impressive text generation abilities and has been used in various applications, including content creation, chatbot development, and code writing.\n",
       "* **LaMDA (Language Model for Dialogue Applications):** Developed by Google, it focuses on conversational AI and is designed to engage in natural, human-like dialogues.\n",
       "* **BERT (Bidirectional Encoder Representations from Transformers):** Developed by Google, it excels at understanding the meaning of words in context and is widely used in tasks like sentiment analysis and question answering.\n",
       "\n",
       "**Key features:**\n",
       "\n",
       "* **Scale:** LLMs are trained on massive datasets, often billions of words or more.\n",
       "* **Transformers:** Many LLMs use transformer architecture, a powerful neural network architecture that has revolutionized NLP.\n",
       "* **Pre-training:** LLMs are typically pre-trained on a large dataset before being fine-tuned for specific tasks.\n",
       "\n",
       "**Impact and future:**\n",
       "\n",
       "LLMs are rapidly evolving and have the potential to revolutionize various industries, from content creation and customer service to education and research. However, ethical concerns regarding bias, misinformation, and job displacement need to be addressed as these models become more powerful.\n",
       "\n",
       "**In essence, LLMs are powerful AI tools that can understand and generate human-like text, opening up exciting possibilities but also raising important ethical questions.**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "result = llm.invoke(\"What is an LLM?\")\n",
    "\n",
    "print(result.__dict__.keys())\n",
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, we can now define a RAG chain that takes a question, retrieves relevant documents, constructs a prompt, passes it into a model, and parses the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        page_number = doc.metadata[\"page\"] + 1 \n",
    "        content_with_page = f\"Page {page_number}:\\n{doc.page_content}\"\n",
    "        formatted_docs.append(content_with_page)\n",
    "    return \"\\n\\n\".join(formatted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Medical Devices Regulation (Regulation (EU) 2017/746) is a regulation of the European Union that governs the placing on the market and putting into service of medical devices. This regulation defines a medical device as any instrument, apparatus, appliance, software, implant, reagent, material or other article intended by the manufacturer to be used, alone or in combination, for human beings for a specific medical purpose. This definition is found on page 5 of the document. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the medical devices regulation?\"\n",
    "Markdown(rag_chain.invoke(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Temperature | Top P | Response |\n",
       "|-------------|-------|----------|\n",
       "| 0.1 | 0.1  | Harmonized standards are a set of standards that are applied to demonstrate conformity with general safety and performance requirements. They are mentioned in the context of demonstrating conformity with the general safety and performance requirements of a device. This information is found on page 147. \n",
       "| 0.1 | 0.5  | Harmonized standards are standards that are applied to demonstrate conformity with general safety and performance requirements. This is mentioned on page 147, under point (c) of the demonstration of conformity. These standards are used alongside other solutions like CS or other methods to ensure compliance. \n",
       "| 0.1 | 0.9  | Harmonized standards are specific standards used to demonstrate conformity with general safety and performance requirements. They are mentioned alongside other solutions like Conformity Assessment Schemes (CS) on Page 147. The demonstration of conformity must include the precise identity of the controlled documents offering evidence of conformity with each harmonized standard. \n",
       "| 0.5 | 0.1  | Harmonized standards are standards that are used to demonstrate conformity with general safety and performance requirements. They are mentioned in the context of demonstrating conformity with the general safety and performance requirements of a device. This information is found on page 147. \n",
       "| 0.5 | 0.5  | Harmonised standards are a set of standards that are used to demonstrate conformity with general safety and performance requirements. These standards are referenced in the technical documentation to offer evidence of conformity. This information can be found on page 147. \n",
       "| 0.5 | 0.9  | Harmonized standards are standards that are applied to demonstrate conformity with general safety and performance requirements. This information is found on Page 147, specifically in point (c) of the \"demonstration of conformity\" section. \n",
       "| 1.0 | 0.1  | Harmonized standards are a set of standards that manufacturers can apply to demonstrate conformity with general safety and performance requirements. This information is found on page 147, specifically in section (c).  Harmonized standards are one of the solutions used to demonstrate conformity, along with other methods like CS or other solutions. \n",
       "| 1.0 | 0.5  | Harmonised standards are standards that are applied to demonstrate conformity with general safety and performance requirements. This information is found on page 147, specifically in point (c) of the demonstration of conformity. These standards are used to ensure the safety and performance of medical devices throughout their lifetime. \n",
       "| 1.0 | 0.9  | Harmonised standards are standards that are applied to demonstrate conformity with the general safety and performance requirements of a device. They are mentioned in the context of the demonstration of conformity, which includes the method or methods used to demonstrate conformity with each applicable general safety and performance requirement. This information is found on page 147 of the document. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures = [0.1, 0.5, 1.0]\n",
    "top_ps = [0.1, 0.5, 0.9]\n",
    "\n",
    "results = \"| Temperature | Top P | Response |\\n\" + \"|-------------|-------|----------|\\n\"\n",
    "\n",
    "for temperature in temperatures:\n",
    "    for top_p in top_ps:\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=temperature, top_p=top_p)\n",
    "\n",
    "        query = \"What is harmonised standards?\"\n",
    "        response = rag_chain.invoke(query)\n",
    "\n",
    "        results += f\"| {temperature} | {top_p}  | {response}\"\n",
    "\n",
    "Markdown(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini vs GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2', max_length=1000, pad_token_id=50256, return_full_text=False)\n",
    "gpt2 = HuggingFacePipeline(pipeline=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "This Regulation provides for prescribing devices with a prescribed medical and\n",
       "\n",
       "medical practice, particularly for an accident or serious injury. The following are\n",
       "\n",
       "1) the technical conditions which are intended to apply after approval from the Technical (Parties)\n",
       "\n",
       "(Chapter 1) Commission.\n",
       "\n",
       ". 2) the conditions and the procedures that will in time be prescribed before receiving approval from the Commission. In\n",
       "\n",
       "concealed agreements in force between technical bodies concerning any matters relating\n",
       "\n",
       "to human life‑related health. These agreements may have\n",
       "\n",
       "related provisions covering the protection of persons at the front and\n",
       "\n",
       "back when working on their device. The legal and\n",
       "\n",
       "legal consequences of taking that action, with respect to personal\n",
       "\n",
       "health, including the risk it might, would\n",
       "\n",
       "have had to a person when they became sick or injured.\n",
       "\n",
       "3) the conditions by which the regulations apply to device manufacturers,\n",
       "\n",
       "especially in the case of medical devices. It shall be\n",
       "\n",
       "unlawful (Article 6 of Regulation (EU) 2016/39/EC) to have any\n",
       "\n",
       "device device manufacturer or any other organisation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain_gpt2 = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | gpt2\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"What is the medical devices regulation?\"\n",
    "Markdown(rag_chain_gpt2.invoke(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The provided text snippets don't contain information about what an LLM is.  \n",
       "\n",
       "LLM stands for **Large Language Model**.  It's a type of artificial intelligence that is trained on massive amounts of text data to understand and generate human-like text. \n",
       "\n",
       "The provided text is from various EU regulations and directives, focusing on things like:\n",
       "\n",
       "* **Competency requirements for staff:** Ensuring personnel involved in assessments and verifications are qualified. \n",
       "* **Data protection:** Implementing measures to protect information and personal data.\n",
       "* **Communication requirements:**  Regulations regarding communication with individuals and entities within the EU.\n",
       "* **References to other regulations:**  Listing specific articles from other directives and regulations. \n",
       "\n",
       "These topics are relevant to various fields, but they don't provide any definition or information about LLMs. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is a LLM?\"\n",
    "rag_chain_prompt_tuning = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "Markdown(rag_chain_prompt_tuning.invoke(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a Q&A chatbot that helps to answer the user's questions about a given document. Always follow these rules to answer the question:\n",
    "\n",
    "Use the following pieces of context to answer the questions.\n",
    "If the question is not related to the context, just say it is not related.\n",
    "If you don't know the answer to any of the questions, just say that you don't know, don't try to make up an answer.\n",
    "Always mention in which pages the information you give are found.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_template,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_runnable = RunnableLambda(lambda input: input[\"question\"])\n",
    "chat_history_runnable = RunnableLambda(lambda input: input[\"chat_history\"])\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": question_runnable | retriever | format_docs,\n",
    "        \"question\": question_runnable,\n",
    "        \"chat_history\": chat_history_runnable,\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuestionAnswerLoop():\n",
    "    print(\"Enter your question (type 'quit' to exit): \")\n",
    "    while True:\n",
    "        user_input = input(\"Enter your question (type 'quit' to exit): \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Exiting Q&A chat. Goodbye!\")\n",
    "            break\n",
    "        else:\n",
    "            chat_history.add_user_message(user_input)\n",
    "            response = rag_chain.invoke(\n",
    "                {\n",
    "                    \"question\": user_input, \n",
    "                    \"chat_history\": chat_history.messages\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Add the AI's response to the chat history\n",
    "            chat_history.add_ai_message(response)\n",
    "\n",
    "            # Print the response\n",
    "            print(\"Question: \" + user_input)\n",
    "            print(\"Answer: \" + response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question (type 'quit' to exit): \n",
      "Question: What is the medical devices regulation?\n",
      "Answer: The provided text is an excerpt from the **Medical Devices Regulation (EU) 2017/745**. \n",
      "\n",
      "This regulation sets out the rules for the **safety and performance** of medical devices sold in the European Union. It aims to:\n",
      "\n",
      "* **Ensure high standards of safety and performance** for medical devices.\n",
      "* **Harmonize regulations** across all EU countries.\n",
      "* **Improve transparency** and traceability of medical devices.\n",
      "* **Increase patient safety** by providing better information and oversight.\n",
      "\n",
      "The regulation covers a wide range of devices, including:\n",
      "\n",
      "* Instruments\n",
      "* Apparatus\n",
      "* Appliances\n",
      "* Software\n",
      "* Implants\n",
      "* Reagents\n",
      "* Materials\n",
      "* Other articles\n",
      "\n",
      "The text excerpts highlight key aspects of the regulation, such as:\n",
      "\n",
      "* **Scope:** The regulation does not affect national laws regarding healthcare provision, but it sets rules for medical devices themselves. \n",
      "* **Definitions:** The definition of a \"medical device\" is central to the regulation and includes a broad range of products intended for human health.\n",
      "* **Responsibilities:** The regulation outlines responsibilities for manufacturers, importers, distributors, and competent authorities in ensuring the safety and performance of medical devices.\n",
      "* **Requirements:** The regulation specifies requirements for device design, documentation, testing, labeling, and post-market surveillance.\n",
      "\n",
      "The Medical Devices Regulation is a complex and comprehensive piece of legislation. The text excerpts provide a glimpse into its main objectives and components. \n",
      "\n",
      "Exiting Q&A chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "QuestionAnswerLoop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG limitations\n",
    "\n",
    "A basic RAG architecture will fetch documents from the database based on the similarity to the given user question. However, the user may not reference the document directly, e.g., \"What is in the first page?\" or \"Can you further explain the first question?\".\n",
    "\n",
    "To solve this, we can pass the question and the chat history to the LLM in an initial step to create the question that will be searched in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.schema import LLMResult\n",
    "\n",
    "class PrintLLMOutputCallbackHandler(BaseCallbackHandler):   # This is used so we can see what's going on behind the scenes\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs):\n",
    "        print(\"LLM Generated:\", response.generations[0][0].text)\n",
    "\n",
    "llm_with_logging = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", callbacks=[PrintLLMOutputCallbackHandler()]\n",
    ")\n",
    "\n",
    "metadata_field_info = [ # Setup the metadata that the LLM will be able to filter by\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        type=\"int\",\n",
    "        description=\"The page number of the document\",\n",
    "    )\n",
    "]\n",
    "\n",
    "document_content_description = \"Medical devices regulation\"\n",
    "\n",
    "query_with_history_template = \"\"\"\n",
    "You are an AI assistant that helps a user query a document.\n",
    "The user makes some questions and you create the queries to find the parts of document that are most relevant to the questions.\n",
    "The search will be performed by similarity so you need to provide a query similar to the contents that are in the document.\n",
    "You do not have the context of the document. You will be making the queries based on the questions and history to get the context from the document.\n",
    "This is the chat history of the conversation until now:\n",
    "<history>\n",
    "{chat_history}\n",
    "</history>\n",
    "\n",
    "Take into account the context of the chat history when preparing the query for the following question:\n",
    "Prepare a query for this question. Output only the query as a natural language question.\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "retriever_prompt_template = PromptTemplate.from_template(query_with_history_template)\n",
    "\n",
    "improved_retriever = SelfQueryRetriever.from_llm(\n",
    "    llm_with_logging,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Ask the LLM to generate a question based on the chat history and the user question\n",
    "# And ask the LLM to perform a search (can filter) based on the question generated\n",
    "improved_retriever_chain = retriever_prompt_template | llm_with_logging | StrOutputParser()\n",
    "\n",
    "# Normal QA RAG chain with the documents from the retrieval\n",
    "improved_rag_chain = question_answering_prompt | llm_with_logging | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_chat_history():   # Format the chat history to be used in the initial prompt (that will generate the database question)\n",
    "        result = \"\"\n",
    "        question_id = 1\n",
    "        for message in chat_history.messages:\n",
    "            if message.type == \"human\":\n",
    "                result += f\"{question_id}. Human: {message.content}\\n\"\n",
    "                question_id += 1\n",
    "            else:\n",
    "                result += f\"AI: {message.content}\\n\"\n",
    "        return result\n",
    "\n",
    "def invoke_improved_rag_chain(user_input):\n",
    "    llm_db_question = improved_retriever_chain.invoke({\"question\": user_input, \"chat_history\": formatted_chat_history()})\n",
    "    docs = improved_retriever.invoke(llm_db_question)\n",
    "    return improved_rag_chain.invoke(\n",
    "        {\n",
    "            \"context\": docs,\n",
    "            \"question\": llm_db_question,\n",
    "            \"chat_history\": chat_history.messages,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What is in the first page of the document?\",\n",
    "    \"Can you further explain the first question?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal RAG Chain\n",
      "\n",
      "\n",
      "Question: What is in the first page of the document?\n",
      "Answer: Unfortunately, I can't tell you what's on the first page of the document. You've provided me with snippets from different pages, but not the actual first page. \n",
      "\n",
      "To find out what's on the first page, you'll need to:\n",
      "\n",
      "1. **Locate the full document:**  Do you have a PDF or physical copy of this document?\n",
      "2. **Open the document:**  Find the beginning of the document, usually marked by a page number \"1.\"\n",
      "\n",
      "Once you have the full document, you'll be able to see the contents of the first page. \n",
      "\n",
      "Question: Can you further explain the first question?\n",
      "Answer: Please provide me with the first question you are referring to. I need the actual question to understand what you are asking for further explanation. \n",
      "\n",
      "The text you provided seems to be a correlation table between articles in different regulations. Without the actual question, I cannot provide a helpful answer. \n",
      "\n",
      "Please share the question, and I will be happy to assist you! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_history = ChatMessageHistory() # reset the chat history\n",
    "\n",
    "print(\"Normal RAG Chain\\n\\n\")\n",
    "for user_input in queries:\n",
    "    chat_history.add_user_message(user_input)\n",
    "    response = rag_chain.invoke(\n",
    "        {\"question\": user_input, \"chat_history\": chat_history.messages}\n",
    "    )\n",
    "    chat_history.add_ai_message(response)\n",
    "    print(\"Question: \" + user_input)\n",
    "    print(\"Answer: \" + response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each query will generate 3 `LLM Generated` answers.\n",
    "- The first one is the actual question to pass to the database, which serves as a way to include the chat history in this question\n",
    "- The second is the LLM generated query to the database\n",
    "- The third is the actual answer from the LLM to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved RAG Chain\n",
      "\n",
      "\n",
      "Question: What is in the first page of the document?\n",
      "---\n",
      "LLM Generated: What is the content of the first page of the document? \n",
      "\n",
      "LLM Generated: ```json\n",
      "{\n",
      "    \"query\": \"Medical devices regulation\",\n",
      "    \"filter\": \"eq(\\\"page\\\", 1)\"\n",
      "}\n",
      "```\n",
      "LLM Generated: The first page of the document contains the title of the document, which is \"REGULATION (EU) 2017/745 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\", and the subject matter and scope of the regulation, which is to lay down rules concerning the placing on the market, making available on the market or putting into service of medical devices for human use and accessories for such devices in the Union. It also applies to clinical investigations concerning such medical devices and accessories conducted in the Union. This information is found on page 1. \n",
      "\n",
      "---\n",
      "Question: Can you further explain the first question?\n",
      "---\n",
      "LLM Generated: What is the purpose and scope of the REGULATION (EU) 2017/745 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL? \n",
      "\n",
      "LLM Generated: ```json\n",
      "{\n",
      "    \"query\": \"purpose and scope of REGULATION (EU) 2017/745 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\",\n",
      "    \"filter\": \"NO_FILTER\"\n",
      "}\n",
      "```\n",
      "LLM Generated: The first page of the document lists the following information:\n",
      "\n",
      "* **Title:** \"REGULATION (EU) 2017/745 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\" \n",
      "* **Subject:** This regulation sets rules for the placing on the market, making available on the market or putting into service of medical devices for human use and accessories for such devices in the Union. \n",
      "* **Scope:** The regulation also applies to clinical investigations concerning such medical devices and accessories conducted in the Union. \n",
      "\n",
      "This information is found on page 1 of the document. \n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "chat_history = ChatMessageHistory()  # reset the chat history\n",
    "\n",
    "print(\"Improved RAG Chain\\n\\n\")\n",
    "for user_input in queries:\n",
    "    print(\"Question: \" + user_input)\n",
    "    print(\"---\")\n",
    "    chat_history.add_user_message(user_input)\n",
    "    response = invoke_improved_rag_chain(user_input)\n",
    "    chat_history.add_ai_message(response)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "This different architecture does not work 100% of the time, but it is a good improvement to the basic RAG architecture.\n",
    "\n",
    "In fact, this new architecture should be able to answer all of the questions that the basic RAG architecture can answer, and also some additional questions that the basic RAG architecture cannot answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
